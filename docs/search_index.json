[["index.html", "A Gentle Introduction to Inference Chapter 1 The Goal", " A Gentle Introduction to Inference Carlisle Rainey 2025-11-13 Chapter 1 The Goal A useful, careful, and intuitive introduction to inference. "],["the-basics-of-probability.html", "Chapter 2 The Basics of Probability 2.1 Review 2.2 Chance Processes 2.3 Defining Probability 2.4 Computing Probabilities 2.5 Conditional Probabilities 2.6 Multiplication Rule 2.7 Independence 2.8 Summary 2.9 Additional Exercises", " Chapter 2 The Basics of Probability 2.1 Review Before we can develop a theory of probability, we need a good understanding of proportions and percents. 2.1.1 Proportions To compute a proportion, we simply count the number of items that fall into a particular category and divide that by the total number of items. For example, if I toss a coin 10 times and got heads-tails-heads-heads-heads-tails-heads-tail-tails-heads, what proportion were heads? In this case, the proportion of heads is \\(\\dfrac{\\text{number tosses that are heads}}{\\text{total tosses}} = \\dfrac{6}{10} = 0.6\\). For another example, suppose I gave 10 As, 13 Bs, 16 Cs, 5 Ds, and 2 Fs in a class. What proportion of student did I give a B? In this example, the proportion of Bs is \\(\\dfrac{\\text{number Bs}}{\\text{total students}} = \\dfrac{13}{10 + 13 + 16 + 5 + 2} = 0.28\\) (rounded to the two decimal-places). In most cases, it makes sense to round proportion to the nearest two decimal-places. 2.1.2 The Indicator Trick It turns out that we can easily connect a proportion to an average using the indicator trick. If we flag all outcomes in a category using the number 1 and all other events using the number 0, then the average of those indicators equals the proportion. In an example above, I tossed a coin 10 times and got heads-tails-heads-heads-heads-tails-heads-tail-tails-heads. We computed the proportion of heads as \\(\\dfrac{\\text{number tosses that are heads}}{\\text{total tosses}} = \\dfrac{6}{10} = 0.6\\). However, we could alternatively flag (or “indicate”) all heads with 1 and all not-heads with 0, giving us the indicators 1, 0, 1, 1, 1, 0, 1, 0, 0, 1. If we just find the average of this list, we get \\(\\dfrac{\\text{sum of list}}{\\text{number of entries in list}} = \\dfrac{6}{10} = 0.6\\). Notice that the indicator trick allows us to “count” by “summing” (or “adding”). The indicator variable is helpful, because we can think of a proportion as an average. Anything that we say about an average must then also be true for a proportion. Exercise 2.1 Suppose I asked 10 people whether they were Republican, Democrat, or Independent. I obtained the follwing: D, R, I, D, D, I, D, I, I, R. Using the indicator trick, what proportion are Republican? Democrat? Independent? Without using the indicator trick (i.e., by counting the number of items that fall into a particular category and then dividing that by the total number of items), do you get the same answers as the exercise above? Explain. Solution 0.2 0.4 0.4 Yes, because noting that an item falls into a category and counting it is the same as assigning it the value 1 and summing it. Similarly, noting that an item does not fall into a category and not counting it is the same as assigning it the value 0 and summing it. Therefore, the indicator trick is the identical to computing the proportion using counting. Exercise 2.2 Suppose I roll a die 10 times and recorded the results. I got 4-2-2-2-1-6-5-6-4-2. Use the indicator trick to compute the proportion of the 10 rolls that showed 6 dots. What if I got 6-3-6-6-6-4-6-6-2-3? Solution To use the indicator trick here, you just need to change the 6s to 1s and the not-6s to 0s. Then average the indicators. 0-0-0-0-0-1-0-1-0-0; avg = 2/10 = 0.2 1-0-1-1-1-0-1-1-0-0; avg = 6/10 = 0.6 2.1.3 Percents To convert a proportion to a percent, simply multiply the proportion (or the fraction) times 100%. If we convert 0.2 to a percent, we get \\(0.6 \\times 100\\% = 60\\%\\). If we convert 0.28 to a percent, we get \\(0.28 \\times 100\\% = 28\\%\\). Fractions, proportions, and percents are different way’s of talking about the same quantities. They have different strengths: Fractions are the easiest to work with when doing math with a pencil. Proportions are the easiest to work with when doing math on a calculator. Percents are the easiest to talk and write about. Make a special note, though, that “%” means “per 100” or “/100.” Because of this, you cannot mindlessly multiply percents. Notice that \\(10\\% \\times 20\\%\\) does NOT equal \\(200%\\). Instead it equals \\(10\\% \\times 20\\% = 10/100 \\times 20/100 = 200/10,000 = 2/100 = 2\\%\\). While you can safely add percents, a good rule is to do calculations until your final answer with fractions or proportions, and only convert to a percent once you have your final answer. Exercise 2.3 Compute the following proportions and percents: Suppose I asked 10 people whether they were Republican, Democrat, or Independent. I obtained the follwing: D, R, I, D, D, I, D, I, I, R. What proportion are Republican? Democrat? Independent? What are the percents for each? Suppose I sample 400 adults living in the US. 180 respondents report they approve of the job Donald Trump is doing as president, 200 report they disapprove, 20 aren’t sure. What proportion approve? Disapprove? Aren’t sure? What are the percents for each? Solution 0.2, 0.4, 0.4; 20%, 40%, 40%. 0.45, 0.50, 0.05; 45%, 50%, 5%. Exercise 2.4 An urn has 5 red marbles, 3 white marbles, and 2 blue marbles. What proportion of the marbles in the urn are red? What fraction? What percent? Suppose I draw 5 marbles from the urn with replacement and recorded the results: R, B, R, W, R. Use the indicator trick to compute fraction, proportion, and percentage of the draws that are red. Solution In the urn: There are 5 red marbles and 10 total marbles. Remember, “to compute a proportion, we simply count the number of items that fall into a particular category and divide that by the total number of items.” \\(\\frac{\\text{number of red marbles}}{\\text{number of marbles, total}} = \\frac{5}{10} = \\frac{1}{2} = 0.5 = 50\\%\\). To convert a fraction or proportion to a percent, we just multiply it by 100%. In this case. \\(\\frac{1}{2} \\times 100\\% 0.2 \\times 100\\% = 50\\%\\). In the sample: Using the indicator trick, we would indicate the colors we want to count with a 1 and the others with a zero. This turns the draws R, B, R, W, R into 1, 0, 1, 0, 1. If we take the average of this list, we have \\(\\frac{3}{5} = 0.6 = 60%\\). 2.2 Chance Processes Define a chance process as a process that one can repeat (independently and under the same conditions) to produce a result from defined set of possible outcomes. We could imagine rolling a die or tossing a coin. We could imagine drawing cards with or without replacement. When we draw with replacement, we draw multiple times, but replace the draws before continuing. This means that same cards can be drawn multiple times. When we draw without replacement, we draw multiple times, but do not replace the draws before continuing. This means that same card cannot be drawn again. Here are several chance processes: Toss a coin and record the side showing. The possible outcomes are heads and tails. Roll a six-sided die and record the number of dots showing. The possible outcomes are 1, 2, 3, 4, 5, and 6. Draw five cards from a well-shuffled deck and record the cards in the order drawn. There are many, many possible outcomes (\\(52 \\times 51 \\times 50 \\times 49 \\times 48 = 311,875,200\\)) Draw two-marbles without replacement from a well-mixed urn contain 5 red marbles, 3 blue marbles, and 2 green marbles. Record the colors of the first and second marbles. There are several possible outcomes. There are 250 million adults living in the US and 100 million approve of the job Donald Trump is doing as president. Select one person at random and record whether they approve of the job Donald Trump is doing as president. The possible outcomes are approve and not (“not” includes those those who “disapprove” and “aren’t sure”). Continue the previous chance process, but select 400 people at random. Record whether each approves of the job Donald Trump is doing as president. There are many possible outcomes. I’ll use the 52-card deck in a lot of my examples. It allows some variety, and many students are already familiar with it. If you are not, then you should learn about the cards that make up a 52-card deck. I find the image below (from Wikipedia) especially helpful. Perhaps print this image and keep it handy. Exercise 2.5 Come up with your own chance process. What is the process? What are the possible outcomes? Solution Here’s a few of examples: Roll a four-sided die and record the number of dots showing. The possible outcomes are 1, 2, 3, and 4. Roll two-six sided die and record the sum of the dots showing. The possible outcomes are 2, 3, 4, …, 11, 12. Draw five cards from a well-shuffled deck without replacement. Record the number of red cards drawn. The possible outcomes are 0, 1, 2, 3, 4, and 5. 2.2.1 The Numbered-Ticket Model And here’s a really important change process. The numbered-ticket model: Fill a box with \\(k\\) tickets numbered \\(t_1\\), \\(t_2\\), \\(t_3\\), …, \\(t_k\\). Draw \\(N\\) times with replacement from the box. Record the average of the draws. For any particular numbered-ticket model, you choose (1) how many tickets go into the box, (3) what numbers go on those tickets, and (3) how many times to draw from the box. We always draw with replacement, and we always average the draws. Exercise 2.6 Write down a particular version of the numbered-ticket model. Do to this, choose how many tickets will go in the box (that’s \\(k\\)), the numbers those tickets will have (that’s \\(t_1\\), \\(t_2\\), \\(t_3\\), …, \\(t_k\\); repeats allowed), and the number of times you will draw with replacement (that’s \\(N\\)). Solution Your solution will vary, but here’s an example: A particular numbered-ticket model: Fill a box with 3 tickets numbered 4, -17, and 100. Draw 9 times with replacement from the box. Record the average of the draws. Notice that we always draw with replacement and we always average the draws. Exercise 2.7 In your own words, explain the numbered-ticket model in a way that someone who hasn’t taken the class could understand. What can change across different versions? What stays the same? 2.3 Defining Probability Now we analyze chance processes using frequentist theory. Under frequentist theory, we’re interested in the probability that a chance process produces some event. For compactness, we sometimes write “the probability that \\(A\\) happens” as \\(\\Pr(A)\\). For example, if we’re tossing a coin, we might care about the probability of a head. We could write this as \\(\\Pr(\\text{head})\\). For another example, suppose we deal five cards from a 52-card deck. We might care about the probability of five hearts. We could write this as \\(\\Pr(\\text{five hearts})\\). In order to define probability in the frequentist theory, we must imagine repeating the chance process over-and-over an infinite number of times. Of course, this is totally hypothetical–we can’t actually do that. But if we did, we could simply compute the proportion of times an event occurred in those infinite repetitions–that’s the probability of that event. Define the probability of an event as the proportion of times the event occurs in the long-run. 2.4 Computing Probabilities 2.4.1 Counting the Equally-Likely Outcomes In order to compute the probability of an event, you first need to list all the equally-likely outcomes of the experiment. If you cannot list the equally-likely outcomes, then you cannot compute the probabilities. For example, if you roll a six-sided die, then the equally-likely outcomes are 1, 2, 3, 4, 5, and 6. If you toss a coin, then the equally-likely outcomes are heads and tails. If you draw from an urn of with 10 marbles, then each marble is equally likely. If you draw from a box of numbered tickets, the each ticket is equally likely. Once we list all the equally-likely outcomes, we simply compute the proportion of outcomes that fall into the category of interest. I refer to this method of computing probabilities as “counting”. You count the outcomes of interest, you count the total outcomes, and you compute the proportion. Examples: Imagine rolling a six-sided die. Compute the probability of getting a 6. First, list the equally-likely ways of 1, 2, 3, 4, 5, and 6. Then compute the proportion that are 6, which is 0.17. So the probability of getting a 6 is 0.17. Imagine rolling a six-sided die. Compute the probability of getting an odd number. First, list the equally-likely ways of 1, 2, 3, 4, 5, and 6. Then compute the proportion that are odd numbers (1, 3, and 5), which is 0.5.. So the probability of getting an odd number is 0.5. Imagine drawing from an urn with 5 red marbles, 3 blue marbles, and 2 green marbles. Compute the probability of getting a red marble. Each marble is equally-likely, so compute the proportion that are red, which is 0.5. Now for a tricky one: Imagine rolling two six-sided dice and summing the two rolls. Compute the probability of getting a 2. At first, you might list the equally-likely outcomes as 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, and 12. Indeed, these are all the outcomes, but they are not equally-likely. It’s more likely that you’ll get a sum of 7 than 2, for example. It’s hard to list the equally-likely ways here, so let me show you how it would work. You don’t need to know how to do this, but you do need to realize that listing the equally-likely ways isn’t always easy or even possible. If we roll two six-sided dice, then 1-then-1 and 3-then-2 are equally likely. In fact, there are 36 of these equally-likely combinations, which we can list in a little matrix below. The first number in each pair represents the first die; the second number represents the second die. (1, 1) (1, 2) (1, 3) (1, 4) (1, 5) (1, 6) (2, 1) (2, 2) (2, 3) (2, 4) (2, 5) (2, 6) (3, 1) (3, 2) (3, 3) (3, 4) (3, 5) (3, 6) (4, 1) (4, 2) (4, 3) (4, 4) (4, 5) (4, 6) (5, 1) (5, 2) (5, 3) (5, 4) (5, 5) (5, 6) (6, 1) (6, 2) (6, 3) (6, 4) (6, 5) (6, 6) These are the 36 equally likely outcomes of the chance process. Only (1, 1) has a sum of 2, so the probability that the sum equals 2 is \\(\\frac{1}{36} = 0.03\\). On the other hand, 6 outcomes have a sum of 7 (can you find them?), so the probability that the sum equals 7 is \\(\\frac{6}{36} = 0.17\\). Exercise 2.8 Compute the probabilities below: Suppose I draw one ticket at random from a box with four tickets numbered 1, 2, 3, and 3. What is the probability that I draw a 1? A 3? Suppose I draw a card from a 52-card deck. What is the probability that I draw a heart? A red card? A 4? The ace of spades? If I toss a coin twice, what’s the probability that the first toss is a head? What’s the probability that the second toss is a head? Suppose I draw once from an urn with 10 red marbles, 7 blue marbles, and 3 green marbles. Compute the probability of getting a red marble? Blue? Green? What if there were 100 red marbles, 70 blue marbles, and 30 green marbles? Solution 0.25, 0.5. 0.25, 0.5, 0.08, 0.02. 0.5, 0.5. 0.50, 0.35, 0.15; The answers don’t change because the ratios of the marbles don’t change. 2.4.2 The Probability of the Opposite Remember that we can compute probabilities by “counting”: List all the equally-likely outcomes. Compute the proportion of outcomes that fall into the category of interest. This means that probabilities can be no smaller than zero and no larger than one. In turn, this implies that the probability that something does not happen equals one minus the probability it happens. Suppose an event \\(A\\). Then \\(\\Pr(\\text{not }A) = 1 - \\Pr(A)\\) and \\(\\Pr(A) = 1 - \\Pr(\\text{not }A)\\). This is a handy rule. It turns out that it’s sometimes really tedious to compute the probability of \\(A\\), but easy to compute the probability of \\(\\text{not } A\\). Exercise 2.9 Compute the probabilities below using the probability of the opposite: Suppose I draw one ticket at random from a box with four tickets numbered 1, 2, 3, and 3. What’s the probability that the draw is not 1? Suppose I draw a card from a 52-card deck. What is the probability that the draw is not the ace of spades? In both of these examples, you could find the probability by counting without the probability of the opposite. But I think using the probability of the opposite makes the calculation somewhat easier. Later, we’ll see some examples where the probability of the opposite makes the problem much easier. Solution 0.75 0.98 2.5 Conditional Probabilities For some chance processes, probabilities can change depending on what’s happened before. Suppose that I draw two cards from a 52-card deck without replacement. The probability of an ace on the first draw is \\(\\frac{4}{52} = 0.08\\). However, if I get an ace on the first draw, then chance of getting an ace on the second draw changes–it’s now lower. I do not get an ace on the first draw, then chance of getting an ace on the second draw still changes–it’s now higher. Here’s the key point: For some chances processes, probabilities can change depending on what’s happened before. If you suspect that the probabilities might have changed, you need to recount the outcomes. This is especially true when sampling without replacement. Let’s go back to the previous example. If I get an ace on the first draw, then the probability of also getting an ace on the second draw is \\(\\frac{\\text{3 aces left}}{\\text{51 cards left}} = 0.05\\). I just removed one ace and recounted all the equally-likely outcomes. If I do not get an ace on the first draw, then the probability of getting an ace on the second draw is \\(\\frac{\\text{4 aces left}}{\\text{51 cards left}} = 0.08\\). (This probability is the same due to rounding error.) I just removed one non-ace and recounted all the equally-likely outcomes. We refer to these new probabilities that depend on what’s happened before as conditional probabilities. We can compactly write the “probability of \\(A\\) given that \\(B\\) has happened” as \\(\\Pr(A \\mid B)\\). Note that conditional probabilities are not difficult. They simply require you to recognize that things might have changed and recount everything. When we sample without replacement, things usually change. For clarity, I sometimes emphasize that a probability is not conditional by calling it an unconditional probability. Exercise 2.10 Compute the probabilities below: Suppose I draw two tickets at random without replacement from a box with four tickets numbered 1, 2, 3, and 3. What is the probability that I draw a 1 on my second draw, given that I draw a 2 on the first draw? What if I draw a 1 on the first draw? What if you draw the two tickets with replacement? Suppose I draw two cards from a 52-card deck without replacement. If I draw a heart on my first draw, what is the probability that I draw a black card on my second draw? Suppose I draw five cards from a 52-card deck without replacement. If the first four cards are draw are red, what’s the probability that the fifth card is black? If I toss a coin five times, what’s the probability that the fifth toss is a head given that the first four tosses are also heads? Solution 0.33, 0, 0.25, 0.25. 0.51. 0.54 0.5 Exercise 2.11 Suppose I draw five times without replacement from an urn with 10 red marbles, 7 blue marbles, and 3 green marbles. What’s the probability that the first draw is green? If the first four draws are red-red-green-green, what’s the probability that the fifth draw is green? How much did the probability change? What if, instead, you used an urn with 1,000 red marbles, 700 blue marbles, and 300 green marbles? What if, instead, you sampled with replacement from the orginal, small urn with 20 marbles? What if, instead, you sampled with replacement from the alternative, large urn with 2,000 marbles? Fill in the blanks with “large” and “small”: When the number of marbles in the urn is small relative to the number of draws, there is a ______ difference between sampling with and without replacement. But when the number of marbles in the urn is large relative to the number of draws, there is a ______ difference between sampling with and without replacement. Solution 0.15, 0.06. 0.15, 0.15. (These are exactly equal because of rounding error, but they are still really close.) 0.15, 0.15. (These are exactly equal.) 0.15, 0.15. (These are exactly equal.) “When the number of marbles in the urn is small relative to the number of draws, there is a large difference between sampling with and without replacement. But when the number of marbles in the urn is large relative to the number of draws, there is a small difference between sampling with and without replacement.” 2.6 Multiplication Rule Suppose I’m interested in the probability that two things both happen. I draw a card that is red and a king. I toss a head and then another head. I draw a red marble and then another red marble. Notice the word “and”–this means there are two things and I’m interested in both happening. When you want to compute the probability that two things both happen, you simply multiply the probability of the first times the probability of the second, given the first. More compactly, \\(\\Pr(A \\text{ and } B) = \\Pr(A) \\times \\Pr(B \\mid A)\\). Remember these two things: The word “and” is a big hint to use the multiplication rule. The multiplication rule is an unconditional probability times a conditional probability. The second point is key: any time you use the multiplication rule, you need to recount everything to make sure they probabilities have not changed. (Again, they most likely will change when sampling without replacement.) Exercise 2.12 Suppose I draw twice without replacement from an urn with 10 red marbles, 7 blue marbles, and 3 green marbles. What’s the probability of getting a red mable and then getting a agreen marble? Suppose I draw twice from a 52-card deck. What’s the probability of getting two aces? (Round this answer to four decimal-places.) Solution 0.08 0.0045 We can easily extent the logic of the multiplication rule to more than two events: The probability that three things all happen is the product of… The probability of the first. The probability of the second, given the first. The probability of the the third, given the first and second. More compactly, we can say that for three events \\(A\\), \\(B\\), and \\(C\\), \\(\\Pr(A \\text{ and } B \\text{ and } C) = \\Pr(A) \\times \\Pr(B \\mid A) \\times \\Pr(C \\mid A \\text{ and } B)\\). You can see the pattern. Exercise 2.13 Suppose I deal a series of cards off the top of a well-shuffled deck without replacement. Some of these probabilies get quite small, so keep around two non-zero digits. If I deal one card, what’s the probability that it’s a heart? If I deal two cards, what’s the probability that both are hearts? If I deal three cards, what’s the probability that all three are hearts? If I deal four cards, what’s the probability that all four are hearts? If I deal five cards, what’s the probability of Ace-King-Queen-Jack-10 (in that order)? Solution 0.25 0.059 0.013 0.0026 0.0000041 2.7 Independence We say that two events are independent if the probability of the second is the same, regardless of how the first turns out. Else, the two events are dependent. More compactly, we can say that events \\(A\\) and \\(B\\) are independent if \\(\\Pr(A) = \\Pr(A | B)\\). Importantly for our purposes, we can say the following: When drawing without replacement, the draws are dependent. When drawing with replacement, the draws are independent. We assume that coin tosses are independent. Same for die rolls. Exercise 2.14 In your own words, explain why without-replacement creates dependent draws, but with-replacement creates independent draws. Use an example with cards, marbles, or numbered tickets to illustrate. Exercise 2.15 Suppose the following numbered-ticket model: Fill a box with 4 tickets numbered -5, -5, 10, and 10. Draw 3 times with replacement from the box. Record the average of the draws. What’s the probability that the average is 10? Would the question be harder or easier if you drew without replacement? Why? Can you state a simpler version of the multiplication rule that applies only to indepedent events? Hint Notice that the only way for the average to equal 10 is for the three draws to all equal 10 (i.e., 10 and 10 and 10). Solution Because we want to know the probability of “10 and 10 and 10,” this requires the multiplication rule. \\(\\Pr(\\text{10 on 1st draw}) \\times \\Pr(\\text{10 on 2nd} \\mid \\text{10 on 1st}) \\times \\Pr(\\text{10 on 3rd} \\mid \\text{10 on 1st and 10 on 2nd} ) =\\) \\(0.5 \\times 0.5 \\times 0.5 = 0.13.\\) Harder, because the probabilities would change at each step because they are dependent. If two events are independent, they you can simply multiply their unconditional probabilities–there is no need to recount to figure out the unconditional probability. In this example, that would be just \\(\\Pr(\\text{10}) \\times \\Pr(\\text{10}) \\times \\Pr(\\text{10})\\). Exercise 2.16 Suppose I toss a fair coin three times. What’s the probability that I get H-H-H (heads, then heads, then heads)? What’s the probability that I get T-T-T? Hint Because these three tosses are independent, you can use the simpler version of the multiplication rule. Solution 0.13 0.13 Exercise 2.17 Suppose I toss a fair coin ten times. What’s the probability that I get all heads? What’s the probability that I get all tails? What’s the probability that I get H-H-T-T-H-H-T-H-T-T? Solution 0.00098 0.00098 0.00098 2.8 Summary A chance process is a process that one can repeat (independently and under the same conditions) to produce a result from defined set of possible outcomes. We use the frequentist theory of probability to analyse change processes. The probability of an event is the proportion of time it occurs in the long-run. To compute a probability, we can list all equally-likely outcomes and compute the proportion of those equally-likely outcomes that fall into the category of interest. The probability of and event is at least zero and at most one. Alternatively, for some event \\(A\\), \\(0 \\leq \\Pr(A) \\leq 1\\). The probability of an event not happening is one minus the chance it happens. Alternatively, for some event \\(A\\), \\(\\Pr(\\text{not-}A) = 1 - \\Pr(A)\\). The probability that two events both happen is the probability of one times the conditional probability of the other. Suppose two events \\(A\\) and \\(B\\). To compute the probability that \\(A\\) and \\(B\\) both happen, we multiply the probability that \\(A\\) happens times the probability of that \\(B\\) happens given that \\(A\\) happens. Or simply \\(\\Pr(A \\text{ and } B) = \\Pr(A) \\times \\Pr(B | A)\\). We say that two events \\(A\\) and \\(B\\) are independent if the probability of A does not depend on whether \\(B\\) happens. Alternatively, two events \\(A\\) and \\(B\\) are independent if \\(\\Pr(A \\text{ and } B) = \\Pr(A) \\times \\Pr(B)\\) (note that the second term on the right-hand side is ununconditional). When you sample with replacement, the draws are independent. When you sample without replacement, the draws are dependent. 2.9 Additional Exercises Exercise 2.18 I roll a die 10 times. Find the probability of… getting 10 sixes. not getting 10 sixes. all the rolls showing five or less (i.e., getting 10 not-sixes) (You do not need to do the multiplication, but at least write it out.) Hint Are options (2) and (3) identical? Suppose I got 4-3-3-2-4-6-3-1-1-3. Would that be “not 10 sixes” (2)? Would be “10 not-sixes” (3)? Solution \\(\\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} = \\left( \\frac{1}{6} \\right) ^{10}\\) \\(1 - \\left( \\frac{1}{6} \\right) ^{10}\\), i.e., this is the opposite of the previous event. \\(\\frac{5}{6} \\times \\frac{5}{6} \\times \\frac{5}{6} \\times \\frac{5}{6} \\times \\frac{5}{6} \\times \\frac{5}{6} \\times \\frac{5}{6} \\times \\frac{5}{6} \\times \\frac{5}{6} \\times \\frac{5}{6} = \\left( \\frac{5}{6} \\right) ^{10}\\) "],["NTM.html", "Chapter 3 The Numbered-Ticket Model 3.1 The Generic Form 3.2 “Is Like” 3.3 Summarizing the NTM Process 3.4 Expected Value and Standard Error 3.5 The Long-Run Histogram 3.6 Summary 3.7 Additional Exercises", " Chapter 3 The Numbered-Ticket Model The numbered-ticket model (NTM) is a familiar model that we can use to understand other, unfamiliar chance processes. If we can figure out what particular NTM “is like” an unfamiliar process, then we can quickly (and almost completely) understand that unfamiliar process. The only hard part is to figure out what numbered-ticket model is like the unfamiliar process. Here’s the key: We can use the NTM to understand (or “predict”) the likely values from a chance process (e.g., the average of the spots shown across 100 rolls of a die) without ever experimenting with either. Creating the analogy, though, is not trivial. 3.1 The Generic Form Here’s the generic form of the NTM: Numbered-Ticket Model: Fill a box with \\(k\\) tickets numbered with \\(t_1\\), \\(t_2\\), \\(t_3\\), …, \\(t_k\\). Draw \\(N\\) times with replacement from the box. Record the average of the draws. As you can see, to create a particular NTM, we need to choose: The number of tickets that go into the box. The numbers that are written on the tickets. The number of times to draw from the box. Remember that we always draw with replacement and then average the draws. And here are three particular NTMs with the details filled in: Fill a box with 6 tickets numbered with 1, 2, 3, 4, 5, 6. Draw 10 times with replacement from the box. Record the average of the draws. Fill a box with 6 tickets, five numbered with 0 and one numbered with 1. Draw 10 times with replacement from the box. Record the average of the draws. Fill a box with 2 tickets numbered with 0 and 1. Draw 100 times with replacement from the box. Record the average of the draws. Fill a box with 50 million tickets, 22 million numbered with 0 and 28 million numbered with 1. Draw 900 times with replacement from the box. Record the average of the draws. Exercise 3.1 Write down three particular versions of the numbered-ticket model. Try to be creative. Push the boundaries of allowable numbers of tickets, numbers on the tickets, and numbers of draws. Solution Your solution will vary, but here’s an example: A particular numbered-ticket model: Fill a box with 3 tickets numbered 4, -17, and 100. Draw 9 times with replacement from the box. Record the average of the draws. You can choose any finite, positive integer (1, 2, 3, …) for the number of tickets in the box. You can number a ticket with any finite, real number (e.g., 0, -1.1, 1222, /3, \\(\\pi\\)). You can choose any finite, positive integer (1, 2, 3, …) for the number of draws. Remember that we always draw with replacement and we always average the draws. Exercise 3.2 A box has 100 tickets: 40 tickets numbered with 0 and 60 tickets numbered with 1. You make 100 draws with replacement from the box. First, does the average of the draws equal the proportion of the draws that are 1s? Why? Second, which best describes the average of the draws? It will always equal exactly 0.6. It will usually equal exactly 0.6. It will not often equal exactly 0.6, but it will usually be close. What if you made the draws without replacement? Solution Because of the indicator trick, the average of the draws equals the proportion of the draws that are 1s. The best choice is: “It will not often equal exactly 0.6, but it will usually be close.” To be 100% sure, I repeated this exact process on my computer one million times. The average equaled exactly 0.6 in only 8% of those repetitions. For the average to equal 0.6, you need to draw exactly 60 1s. But 59 or 61 1s happens almost as often as 60. And 58 or 62 1s happens almost as often as 59 and 61. Indeed, you get 55 1s about 5% of the time (same for 65). As such, 0.6 is the most likely outcome, but you get that only about 8% of the time in the long run. If you sampled without replacement, you would always get an average of exactly 0.6, because you sampled the entire box and the average of the box is 0.6. 3.2 “Is Like” The NTM is valuable because it’s very easy to summarize. If we can find an NTM that “is like” an unfamiliar chance process of interest, then we easily summarize that unfamiliar chance process. Below are two key examples that show how it works. 3.2.1 Key Example 1 Rolling a die 10 times and averaging the dots shown across the 10 rolls is like drawing 10 times with replacement from a box with six tickets numbered with 1, 2, 3, 4, 5, 6 and averaging the draws. Read that again and convince yourself that the two are equivalent. On each roll of the die, we have a 1/6 probability of getting a 1, and a 1/6 probability of getting a 2, and so on. These probabilities are identical for the NTM. Sampling with replacement from a box with tickets numbered 1, 2, 3, 4, 5, 6 is just like rolling a die. Exercise 3.3 Get the following materials: A six-sided die. (If you don’t have one handy, here’s a virtual substitute.) A box of six tickets numbered 1, 2, 3, 4, 5, 6. An Ace, 2, 3, 4, 5, and 6 from a deck of cards works great. Now do the following: Roll the die ten times. Record each roll, then compute the average. Draw 10 times with replacement (mixing well after replacement) from the box of tickets. Record each draw, then compute the average. What are your results? You probably got a different average from each, but are these chance processes analogous? If I were going to pay you the average-of-the-rolls or the average-of-the-draws in dollars, would you care which of the two I used (before you saw the results)? 3.2.2 Key Example 2 Rolling a die 10 times and computing the proportion of sixes is like drawing 10 times with replacement from a box with six tickets numbered with 0, 0, 0, 0, 0, 1 and averaging the draws. Read that again and convince yourself that the two are equivalent. On each roll of the die, we have a 5/6 probability of getting a not-six and a 1/6 probability of getting a six. On each draw from the box, we have a 5/6 probability of drawing a 0 and a 1/6 probability of drawing a 1. Because of the indicator trick, getting a six and counting is just like drawing a 1 and summing it. Similarly, getting a not-six and not counting it is just like drawing a zero and summing it. Because of the indicator trick, sampling with replacement from a box with six tickets numbered with 0, 0, 0, 0, 0, 1 and summing the draws is just like rolling a die and counting the sixes. (Then we could convert the count/sum to a proportion/average by dividing by the number of rolls/draws.) Here’s a guideline: When the chance process involves a proportion (e.g., the proportion of sixes), your box of tickets needs to have only 0s and 1s. It needs 1s for the things that count and 0s for the things that don’t. I refer to a box of tickets that contains only 0s and 1s as a 0-1 box (read as “zero one box”). Exercise 3.4 Get the following materials: A six-sided die. (If you don’t have one handy, here’s a virtual substitute.) A box of six tickets numbered 0, 0, 0, 0, 0, 1. Now do the following: Roll the die 10 times. Record each roll, then compute the proportion of the rolls that are six. Draw 10 times with replacement (mixing well after replacement) from the box of tickets. Record each draw, then compute the average. What are your results? Your proportion and average are probably not the same, but are these chance processes analogous? If I were going to pay you the proportion-of-sixes or the average-of-the-draws in dollars, would you care which of the two I used (before you saw the results)? Exercise 3.5 You can play either of the following games of chance: Win $10 if you roll a six with a six-sided die. Draw once from a box of six tickets numbered with 0, 0, 0, 0, 0, 10 and win the amount on the ticket, in dollars. Which would you choose? Or are they the same? Solution They are the same. With both options, you have a 1/6 chance of winning $10. The objects used aren’t relevant. Exercise 3.6 You can play either of the following games of chance: Draw once from a box of six tickets numbered with 0, 0, 0, 100, 100, 100. You win the amount shown on the ticket. Draw once from a box of two tickets numbered with 0 and 100. You win the amount shown on the ticket. Which would you choose? Or are they the same? Solution They are the same. With both options, you have a 3/6 = 1/2 chance of winning $100. The first option has more tickets, but because we’re sampling with replacement, the two games are equivalent. Exercise 3.7 Choose an NTM that “is like” the following unfamiliar chance processes. Tossing a fair coin 100 times and computing the proportion of heads is just like drawing ______ times with replacement from a box with _____ tickets numbered with ________ and averaging the draws. Suppose you have a biased coin that comes up heads 60% of the time. Tossing this biased coin 100 times and computing the proportion of heads is just like drawing ______ times with replacement from a box with _____ tickets numbered with ________ and averaging the draws. Suppose you have a biased coin that comes up heads 50.1% of the time. Tossing this biased coin 100 times and computing the proportion of heads is just like drawing ______ times with replacement from a box with _____ tickets numbered with ________ and averaging the draws. Solution 100; 2; 0, 1. 100; 10; 6 1s and 4 0s. (Alternatively, 100; 100; 60 1s and 40 0s. The key is to make 60% of the box 1s and the rest 0s.) 100; 1000; 501 1s and 499 0s. The key is to make 50.1% of the box 1s and the rest 0s. Exercise 3.8 Suppose you have an urn with 6 red marbles, 3 blue marbles, and 1 yellow marble. Drawing 5 marbles with replacement and computing the proportion that are blue is like drawing ______ times with replacement from a box with _____ tickets numbered with ________ and averaging the draws. Solution 5; 10; 3 1s and 7 0s The key is to make 30% of the box 1s, just like 30% of the marbles in the urn are blue. 3.3 Summarizing the NTM Process As an example, suppose that we draw 10 times with replacement from a box of six tickets numbered with 1, 2, 3, 4, 5, 6 and average the draws (Key Example #1). This is analogous to rolling a die 10 times and averaging the spots on each roll. Let’s think about how we can summarize (or “predict”) this chance process. For an NTM, the average-of-the-draws is random, just like a coin toss. You don’t know exactly what you’ll get. Sure, if you toss a coin, it might come up heads. But we can describe the coin toss process as having a 0.5 probability of heads. Similarly, we can describe the NTM process. It turns out that we can summarize the random average-of-the-draws from the NTM much like we summarize a list of numbers: The average-of-the-draws will be about __________ give or take __________ or so. 3.3.1 Letting a Computer Do Some Work To better understand this chance process, let’s use a computer to execute the NTM above (draw 10 times with replacement from a box of six tickets numbered with 1, 2, 3, 4, 5, 6 and average the draws). First, create a box with six tickets numbered 1, 2, 3, 4, 5, 6. box &lt;- c(1, 2, 3, 4, 5, 6) Now draw 10 times with replacement from that box (and print the draws). draws &lt;- sample(box, size = 10, replace = TRUE) print(draws) [1] 6 3 2 4 2 5 3 2 6 3 Finally, compute (and print) the average-of-the-draws. average_of_the_draws &lt;- mean(draws) print(average_of_the_draws) [1] 3.6 For our 10 draws, we get 6, 3, 2, 4, 2, 5, 3, 2, 6, 3. The average-of-the-draws is 3.6. For giggles, I repeated the process and drew6, 6, 2, 1, 6, 6, 1, 4, 4, 2, so the second average-of-the-draws is 3.8—slightly different from the first. You see, each time we run the NTM, we get a slightly different average-of-the-draws. This raises an important question: If we ran the NTM again, and again, and again—a huge number of times—what would the average-of-the-draws look like?** I’d like to repeat it an infinite number of times, but that’s impossible. So let’s say that “huge number of times” is 10,000 (and assume that 10,000 is reasonably close to infinite). Let’s repeat this process 9,998 more times. I’m not showing the code, and I’m not printing all 10,000 repetitions, but here’s an abbreviated table. Repetion Draws Average-of-the-Draws #1 6, 3, 2, 4, 2, 5, 3, 2, 6, 3 3.6 #2 6, 6, 2, 1, 6, 6, 1, 4, 4, 2 3.8 #3 4, 3, 1, 6, 5, 4, 2, 6, 2, 4 3.7 #4 3, 4, 1, 1, 4, 3, 6, 1, 5, 3 3.1 #5 1, 1, 5, 2, 2, 3, 3, 5, 3, 1 2.6 #6 2, 3, 1, 3, 4, 2, 4, 3, 5, 6 3.3 #7 3, 2, 2, 5, 6, 4, 4, 2, 5, 4 3.7 #8 3, 5, 3, 4, 1, 3, 1, 3, 2, 2 2.7 #9 2, 4, 4, 2, 4, 5, 1, 2, 2, 1 2.7 #10 4, 1, 6, 1, 3, 2, 5, 2, 6, 1 3.1 … … … #9999 2, 4, 4, 2, 4, 3, 6, 4, 3, 6 3.8 #10000 2, 3, 5, 1, 1, 4, 4, 4, 4, 6 3.4 Now look closely at the “Average-of-the-Draws” column in the table above. Imagine that we took the average of that column and SD of that column. We might think of those as the “long-run” average and SD of the average-of-the-draws. When I compute the average of our 10,000 averages-of-the-draws, I get 3.4932. When I compute the SD, I get 0.5400138. Those not exactly the long-run average and SD, because our “long run” is only 10,000 draws, but they should be very close. We can use these numbers to summarize the NTM with the following: The average-of-the-draws will be about __________ give or take __________ or so. We refer to the first blank as the expected value and the second blank as the standard error (SE). 3.4 Expected Value and Standard Error The expected value is much like an average, except the expected value summarizes a chance process and the average summarizes an observed list of numbers. The SE is much like an SD, except the SE summarizes a chance process and the SD summarizes an observed list of numbers. The table below summarizes the relationship. Quantity Observed List of Numbers Chance Process typical value average expected value give-or-take SD SD Much like causation requires us to imagine a hypothetical, counterfactual world, the expected value and SE require us to imagine repeating the NTM an infinite number of times. EAch repetition gives us one average-of-the-draws, so infinite repetitions gives us an imaginary, infinitely-long list of averages-of-the-draws. The the imaginary average of this imaginary, infinitely-long list gives us the expected value. The SD of this imaginary, infinitely-long list gives us the SD. While understanding the expected value and SE requires a lot of imagination, computing the expected value and SE only requires straightforward formulas. For a numbered-ticket model, the expected value is the long-run average of the average-of-the-draws and equals the average of tickets in the box. For a numbered-ticket model, the SE is the long-run SD of the average-of-the-draws and equals \\(\\dfrac{\\text{SD of the tickets in the box}}{\\sqrt{\\text{number of draws}}}\\). We know how to compute averages and SDs, so it’s really easy to compute the expected value and SE. mean(box) # expected value [1] 3.5 sqrt(mean((box - mean(box))^2))/sqrt(10) # SE [1] 0.5400617 Exercise 3.9 Compute the expected value and SE for the average-of-the-draws for an NTM that uses the following boxes of tickets and numbers of draws. [1, 2, 3, 4, 5, 6]; 100 draws [1, 1, 1, 4, 5, 6]; 9 draws [-17, 4, 3, 6]; 400 draws (Hint: The SD of this box is 9.30.) [100, -14, 17, 3]; 16 draws (Hint: The SD of this box is 43.83.) For each NTM, fill in the blanks: The average-of-the-draws will be about __________ give or take __________ or so. Solution 3.5; 0.17 3; 0.69 -1; 0.47 26.510.96 The expected value belongs the first blank; the SE belongs in the second. Exercise 3.10 Draw _______ times with replacement from a box with six tickets numbered with 1, 2, 3, 4, 5, 6 and averaging the draws. Find the SE for the average-of-the-draws for the following number of draws: 1 9 10 400 900 2500 Keep two digits in addition to the leading zeros. Do you notice any pattern? Was this pattern predictable by simply inspecting the formula for the SE? Solution 1.71 0.57 0.54 0.085 0.034 3.4.1 Tricks for 0-1 Boxes If your box of tickets has only 0s and 1s, then I refer to it as a “0-1 box” (read as “zero one box”). For a 0-1 box, we have a couple of rules for quickly computing the average and SD of the tickets in the box.: The average of the tickets in the box equals the proportion of the tickets numbered with 1. (This is just the indicator trick.) The SD of the tickets in the box equals \\(\\sqrt{(\\text{avg. of the tickets in the box}) \\times (1 - \\text{avg. of the tickets in the box})}\\). This is an incredibly helpful trick. As you know, computing the SD is tedious. But if you’re working with a 0-1 box, computing the SD of the box is easy. Exercise 3.11 Compute the expected value, the SD of the box, and the SE for an NTM that uses the following boxes of tickets and numbers of draws. Notice that each box is a 0-1 box, so use the appropriate tricks. [0, 1]; 9 draws [0, 0, 1]; 400 draws [0, 1, 1]; 16 draws [0, 0, 0, 0, 0, 1]; 900 draws a box with 400 0s and 200 1s; 100 draws a box with 120 million 0s and 180 million 1s; 900 draws For each NTM, fill in the blanks: The average-of-the-draws will be about __________ give or take __________ or so. Solution EV = 1/2 = 0.50; SD of the box = sqrt[0.5 x (1 - 0.5)] = 0.5; SE = 0.5/sqrt(9) = 0.17 EV = 1/3 = 0.33; SD of the box = sqrt[0.33 x (1 - 0.33)] = 0.47; SE = 0.47/sqrt(400) = 0.024 EV = 2/3 = 0.67; SD of the box = sqrt[0.67 x (1 - 0.67)] = 0.47; SE = 0.47/sqrt(16) = 0.12 EV = 1/6 = 0.17; ; SD of the box = sqrt[0.17 x (1 - 0.17)] = 0.38; SE = 0.38/sqrt(900) = 0.013 EV = 200/600 = 1/3 = 0.33; SD of the box = sqrt[0.33 x (1 - 0.33)] = 0.47; SE = 0.47/sqrt(100) = 0.047 EV = (180 million)/(300 million) = 180/300 = 3/5 = 0.60; SD of the box = sqrt[0.6 x (1 - 0.6)] = 0.49; SE = 0.49/sqrt(900) = 0.016 The expected value belongs in the first blank; the SE belongs in the second. Exercise 3.12 Define the expected value and SE. Choose a particular NTM (i.e., choose the number of tickets, the numbers on each ticket, and the number of draws) and use it as an example. What exactly does “long-run average of the average-of-the-draws” mean? What exactly does “long-run SD of the average-of-the-draws” mean? Compute the expected value of the average-of-the-draws for your NTM. Compute the SE of the average-of-the-draws for your NTM. Solution For a numbered-ticket model, the expected value is the long-run average of the average-of-the-draws. The SE is the long-run average of the average-of-the-draws. An example helps illustrate what I mean by “long-run” average and SD. I’m going to use an NTM with and 5 draws and 3 tickets numbered 1, 2, and 3. To conceive of the expected value and SE, we need to imagine repeating this NTM a large number of times. Each repetition produces one average-of-the-(5)-draws. Suppose we repeated it one million times. Then we would have one million averages-of-the-(5)-draws. If we took the average of these one million averages-of-the-(5)-draws, then that’s a reasonable approximation to the expected value or “long-run average of the average-of-the-draws.” If we took the SD, then that’s a reasonable approximation to the SE or “long-run SD of the average-of-the-draws.” In short, to conceive of the expected value and SE, we need to imagine a hypothetical, infinitely long list of averages-of-the-draws that we would obtain by repeating the NTM over-and-over-and-over. The expected value is the average of this hypothetical, infinitely-long list. The SE is the SD of this same list. The expected value is always equal to the average of the tickets in the box. My box has tickets numbered 1, 2, and 3, so the expected value is 2. The SE is always equal to the SD of the box divided by the square root of the number of draws. In my example, that’s 0.37. Exercise 3.13 Use the NTM app to explore your own NTM. Vary the number of draws. Vary the tickets in the box. Be creative. Confirm that the app correctly computes the expected value and the SE. Notice that the app does two things. First, it computes the exact expected value and the SE. Second, it shows you the average-of-the-draws for five repetitions and the histogram of the average of the draws for 10,000 repetitions. Exercise 3.14 Suppose that of the roughly 300 million adults living in the US, 120 million approve of the job Donald Trump is doing as president, 180 disapprove, and 20 aren’t sure. Suppose I sample 400 at random. What’s expected value and SE of the proportion of people that approve. Fill in the blanks: The proportion that approve will be about __________ give or take __________ or so. Hint To find the expected value and SE for this proportion, you need to move from an unfamiliar context (sample survey) to a familiar context (numbered-ticket model). This sample survey is just like drawing 400 times from a box of numbered tickets with 120 million 1s and 180 million 0s. Solution After setting up an analogous NTM, you can easily see that the expected value of the proportion of the sample that approves is (120 million)/(300 million) = 0.4. To compute the SE, we first need the SD of the box. SD of the box = sqrt[0.4 x (1 - 0.4)] = 0.49. The SE = (SD of box)/sqrt(number of draws) = 0.49/sqrt(400) = 0.024. The proportion of people that approve will be about 0.4 give or take 0.024 or so. 3.5 The Long-Run Histogram We already know that the average-of-the-draws in the NTM is the [expected value] give or take the [SE]. This is a fairly complete summary of the process. Indeed, it’s almost magical. But we can describe the process even more completely. It turns out that, as long as the number of draws is large enough, the average-of-the draws follows the normal curve. What?! Let me say that again. No matter what tickets you put in the box, the average-of-the-draws follows the normal curve (as long as the number of draws is large enough). Again, imagine that we repeat the NTM over-and-over-and-over, say 10,000 times. If you take the 10,000 averages-of-the-draws and create a histogram, then that histogram follows the normal curve. This is an amazing, magical result. Exercise 3.15 We’re not going to prove this result (called the “central limit theorem”) mathematically. But you can explore a variety of NTMs to confirm that it works. Use the NTM app to explore several NTMs. Try the following boxes. Start with 10 draws and increase the number of draws until the 10,000 averages-of-the-draws seems roughly normal. (The app can be a little slow when the number of draws gets bigger than about 50, so be patient–it’s repeating the process 10,000 times.) 1, 2, 3, 4, 5, 6 0, 1 0, 0, 0, 0, 0, 1 1, 1, 2, 3, 4, 5, 6000 For a large-enough number of draws, does the average-of-the-draws roughly form a normal curve for each box? Solution 10 draws is definitely large enough 10 draws is definitely large enough about 30 draws is large enough With 10 draws, you get a definitely-not-normal distribution. Each “hump” is created by drawing different numbers of 60s. The left-most hump is no 60s. The second includes all the sets of draws with 1 60. And so on. With 100 draws, the humps are less prominent, but still present. With 400 draws, the average-of-the-draws follows a normal distribution. Because the average-of-the-draws follows the normal curve, we can say the following (assuming the number of draws is large enough): There is a 68% chance (or 0.68 probability) that the average-of-the-draws falls within 1 SE of the expected value. There is a 95% chance (or a 0.95 probability) that the average-of-the-draws falls within 2 SEs of the expected value. We can also use the normal approximation to compute the probability that the average-of-the-draws falls into a particular interval. If the number of draws is large, this approximation is extremely precise (much more precise than for most lists of numbers). To do this, remember the steps for a normal approximation, but use the expected value in place of the average and the SE in place of the SD: Draw a picture. This is important. If you can draw the correct picture, it’s really easy to find the correct normal approximation. Draw the normal curve. Label the points of interest. I find it helpful to label the expected value as well. Shade the area of interest. Convert the points of interest to standard units or z-scores. I like to add the z-scores in parentheses underneath the points of interest. Remember that \\(\\text{std. unit} = z\\text{-score} = \\frac{\\text{value } - \\text{ expected value}}{\\text{SE}}\\). Use the Rules of the Normal Curve. The normal table gives the area between \\(z\\) and its opposite \\(-z\\). See Appendix A of this monograph. Usually, start here. The area under the entire curve is 100%. The curve is symmetric so that the area above a particular value \\(z\\) equals the area below its opposite \\(-z\\). Example Suppose we draw 9 times with replacement from a box with two tickets numbered with 0 and 1 and average the draws. (Because this is a 0-1 box, remember to use the tricks to compute the average and SD of the box.) \\(\\text{expected value} = \\text{average of the box} = \\frac{1}{2} = 0.50\\) \\(SE = \\dfrac{\\text{SD of box}}{\\sqrt{\\text{number of draws}}} = \\dfrac{\\sqrt{0.50 \\times (1 - 0.50)}}{\\sqrt{9}} = \\dfrac{0.5}{3} = \\dfrac{1}{6} = 0.17\\) The average-of-the-draws will fall between 0.33 and 0.67 with probability 0.68 (or 68% of the time). It will fall between 0.16 and 0.84 with o probability of 0.95 (or 95% of the time). We can also compute the probability of getting and average-of-the-draws greater than 0.7. Here’s my work: The expected value is 0.5 and the SE is 0.17. The quantity of interest 0.7 is (0.7 - 0.5)/0.17 = 1.18 SEs above the average. That’s the z-score. If we look up z = 1.18 in the normal table (see Appendix A of this monograph), we see that it isn’t there. Instead, use the closest, which is 1.2. We see that 77% falls between -1.20 and 1.20. If 77% falls “in the middle,” then 23% falls in the two tails. If 23% falls in the two tails, then 11.5% or 12% falls in each tail. Therefore, the probability of obtaining an average-of-the-draws greater than 0.7 is 0.12. Or we might say “the average-of-the-draws will be larger than 0.7 12% of the time.” Exercise 3.16 Suppose an NTM where we’re drawing 100 times from a box with two tickets, one numbered with 0 and the other numbed with 1. There’s a 95% chance that the average-of the draws is between ____ and ____. Solution The expected value is 0.5, the SD of the tickets in the box is 0.5, and the SE is 0.05. Remember that the average-of-the-draws follows the normal curve, the average-of-the-draws falls with in 2 SEs of the expected value 95% of the time. So we just hop 2 SEs below the expected value to find the lower-end of the range. We hop 2 SEs above the expected value to find the higher-end of the range. In this case, the interval is [0.5 - (2 x 0.05), 0.5 + (2 x 0.05)] = [0.4, 0.6]. There’s a 95% chance that the average of the draws is between 0.4 and 0.6. Exercise 3.17 Suppose and urn filled with 5 red marbles, 4 green marbles, and 1 yellow marble. Draw 100 marbles with replacement and compute the proportion that are red. There’s a 95% chance that the proportion is between ____ and ____. Solution This is just like the Exercise 3.16, because the NTM above “is like” this unfamiliar chance process. Exercise 3.18 Suppose and urn filled with 5 red marbles, 4 green marbles, and 1 yellow marble. Draw 100 marbles with replacement and compute the proportion that are red. There’s a ____% chance that the proportion is above 0.58. Solution We can build on our work from Exercise 3.16, because the NTM described in that question “is like” this unfamiliar chance process of drawing marbles from an urn. expected value of the proportion of draws that are red = 0.5 SE of the proportion of draws that are red = 0.05 Now following the normal approximation, we would draw a picture, convert to standard units and then use the rules. The expected value is 0.5 and the SE is 0.05. The quantity of interest 0.58 is (0.58 - 0.5)/0.0 = 1.6 SEs above the average. That’s the z-score. If we look up z = 1.6 in the normal table (see Appendix A of this monograph), we see that 89% falls between -1.6 and 1.6 (i.e., “in the middle”). If 89% falls “in the middle,” then 11% falls in the two tails. If 11% falls in the two tails, then 6.5% or 7% falls in each tail. Therefore, the probability of obtaining a proportion-that-are-red greater than 0.58 is 0.07. Or we might say “the proportion of the draws that are red will be larger than 0.58 7% of the time.” Exercise 3.19 Suppose and urn filled with 8 red marbles, 1 green marble, and 1 yellow marble. Draw 100 marbles with replacement and compute the proportion that are red. There’s a ____% chance that the proportion is less than 0.78. Solution Using the analogous NTM, we can see that: expected value of the proportion of draws that are red = 0.8 the SD of the tickets in the box is sqrt(0.8*0.2) = 0.4 SE of the proportion of draws that are red = 0.4/sqrt(100) = 0.4/10 = 0.04. Now following the normal approximation, we would draw a picture, convert to standard units and then use the rules. The expected value is 0.8 and the SE is 0.04. The quantity of interest 0.78 is (0.78 - 0.80)/0.04 = 0.5 SEs below the average. That’s the z-score. If we look up z = 0.5 in the normal table (see Appendix A of this monograph), we see that 38% falls between -0.5 and 0.5 (i.e., “in the middle”). If 38% falls “in the middle,” then 62% falls in the two tails. If 62% falls in the two tails, then 31% falls in each tail. Therefore, the probability of obtaining a proportion-that-are-red less than 0.78 is 0.31. Or we might say “the proportion of the draws that are red will be lless than 0.78 31% of the time.” Exercise 3.20 Suppose I draw five cards from a deck with replacement and compute the proportion that are hearts. There’s a 95% chance that the proportion will be between _____ and ____. Solution We need to be careful, because the average-of-the-draws follows the normal curve only if the number of draws is “large enough.” We need to use an NTM with 5 draws and a box with 3 0s and a single 1. expected value = 0.25 SD of the box = sqrt(.25*.75) = 0.43 SE = 0.43/sqrt(5) = 0.19 We’re familiar enough with the normal table to know that 95% of the normal curve is between -2 and 2, so we just hop two SEs to the left and two SEs to the right. That’s 0.25 - 2*0.19 and 0.25 + 2*0.19, or -0.13 and 0.63. Something seems to have gone wrong, because the proportion of hearts cannot be negative. This is not as big a problem as it seems, but it stems partly from the fact that we need the number of draws to be “large enough.” Five draws might not be large enough. To better understand, I plugged this NTM into the app, and got the results below. but this isn’t as bad as it initially seems. On my computer, I simulated this NTM 10,000 times and the average-of-the-draws fell between -0.13 and 0.63 98% of the time. That’s not too far off 95%, especially since we would usually consider 5 a “large” number of draws. 3.6 Summary The numbered-ticket model is a particularly useful chance process to understand well, because we can choose the number of tickets, the numbers on those tickets, and the number of draws to make it “like” many other chance processes. When using the familiar numbered-ticket model to understand an unfamiliar chance process, ask: How many tickets go in the box? What number goes on each ticket? How many draws should I take? The expected value is the long-run average of the average-of-the-draws. It’s exactly equal to the average of the tickets in the box. The standard error (SE) is the long-run SD of the average-of-the-draws. It’s exactly equal to \\((\\text{SD of the tickets in the box})/\\sqrt{({\\text{number of draws}})}\\). The expected value and SE are a lot like the average and SD. The difference is key: the expected value and SE describe a chance process, while the average and SD describe observed lists of numbers. When using the numbered-ticket model to understand an unfamiliar chance process involving a proportion, remember to fill your box with 0s and 1s (i.e., use the indicator trick). When working with a 0-1 box (i.e., a box with only 0s and 1s), it’s helpful to remember two tricks: (1) the average of the tickets in the box equals the proportion of tickets that are 1s and (2) the SD of the tickets in the box equals \\(\\sqrt{(\\text{avg. of tickets in the box}) \\times (1 - \\text{avg. of the tickets in the box})}\\). If you have a large number of draws, then the long-run histogram of the average-of-the-draws follows the normal curve. Use this long-run histogram to compute the probability that the average-of-the-draws will fall into particular ranges (e.g., more than 2, between 1 and 2, less than -1, etc.). After reading this chapter and doing these exercises, you should be able to… Describe the concepts of expected value and SE in a simplified manner. Connect a numbered-ticket model to an unfamiliar chance process and describe the (random) outcomes of that chance process as follows. The [outcome of the chance process] will be about __________ give or take __________ or so. About 68% (or 95%) of the time, the [outcome of the chance process] will be between _________ and _________. The [outcome of the chance process] will fall above/below/between ___________ about ____ percent of the time. 3.7 Additional Exercises Exercise 3.21 250 million adults are living in the US. 100 million “approve” of the job Donald Trump is doing as president, 140 million “disapprove,” and 10 million “aren’t sure.” Suppose I sample 400 of these adults and compute the proportion of people that approve. Choose an NTM that “is like” this chance process and use it to answer the questions below. Because the SE is so small for this problem, round to three decimal places. The proportion that approve will be about __________ give or take __________ or so. About 68% of the time, the proportion that approve will be between _________ and _________. About 95% of the time, the proportion that approve will be between _________ and _________. The proportion that approve will fall above 0.42 _________ percent of the time. The proportion that approve will fall below 0.39 _________ percent of the time. The proportion that approve will fall between 0.39 and 0.42 _________ percent of the time. Solution The expected value is 0.4, the SD of the numbers in the box is \\(\\sqrt{0.4 \\times 0.6} = 0.49\\), and the SE is \\(\\frac{0.49}{\\sqrt{400}} = \\frac{0.49}{20} = 0.025\\). The proportion of people that approve will be about 0.4 give or take 0.025 or so. About 68% of the time, the proportion of people that approve will be between 0.375 and 0.425. About 95% of the time, the proportion of people that approve will be between 0.35 and 0.45. The proportion of people that approve will fall above 0.42 21% percent of the time. The proportion of people that approve will fall below 0.39 35% percent of the time. The proportion that approve will fall between 0.39 and 0.42 (100% - 21% - 35%) = 44% percent of the time. (Just using the previous two questions.) Exercise 3.22 In your own words, explain the numbered-ticket model in a way that someone who hasn’t taken the class could understand. What can change across different versions? What stays the same? (Compare this answer to what you wrote at the beginning of the last chapter.) Exercise 3.23 Suppose you have an urn with 6,000 red marbles, 3,000 blue marbles, and 1,000 yellow marbles. Drawing 100 marbles with replacement and computing the proportion of draws that are blue is like drawing ______ times with replacement from a box with _____ tickets numbered with ________ and averaging the draws. But suppose I sampled 100 times without replacement from the urn. Could I use the same NTM to analyze the draws from the urn? Or does the assumption of sampling with replacement completely invalidate the analogy? Solution 100; 10; 3 1s and 7 0s The key is to make 30% of the box 1s, just like 30% of the marbles in the urn are blue. Sampling with and without replacement are a lot alike, so long as the number of draws are small relative to the number of marbles in the urn. In this case, 100 draws is a lot smaller than 10,000 marbles, so we can use the NTM that assumes drawing with replacement to analyze a change process that assumes drawing without replacement. "],["sample-surveys.html", "Chapter 4 Sample Surveys 4.1 A Conceptual Framework 4.2 The Simple Random Sample 4.3 The Three Hurdles 4.4 Good Alternatives to the Simple Random Sample 4.5 Poor Alternatives to the Simple Random Samples 4.6 An Infamous Sample Survey 4.7 Modern Polling 4.8 Summary", " Chapter 4 Sample Surveys 4.1 A Conceptual Framework In this chapter, we apply our knowledge of sampling (i.e., the numbered-ticket model) to sample surveys. Political scientists often ask questions about large populations of people. We refer to the collection of cases of interest as the population. For example, we might care about the attitudes of adults living in the US. In this case, the population is adults living in the US. About 250 million adults are living in the US. Of course, we cannot gather data on each of these 250 million people, even though that is the population of interest. When the population is too large to study as a whole, political scientists sometimes focus on a smaller subset of the population. We refer to this subset of the population for which we gather data as a sample. Usually, political scientists are interested in a particular numerical summary of the population, such as an average or a percent. We refer to the numerical summary of the population as the parameter. We refer to the corresponding summary of the sample as a statistic. But when you do not have data for the entire population, but instead data for a sample, we must address a key question: How closely does the sample statistic resemble the population parameter? For example, we might see that 42% of our sample approve of the job Donald Trump is doing as president, but we want to know what this means about the population. Is the population within one percentage point? Or is it withing 10 percentage points? We refer to the difference between the sample statistic and the population parameter as error. We rarely know the error, because we rarely know the population parameter. It turns out that the process for choosing the sample affects (1) the likely size of the sampling error and (2) our ability to determine the likely size of the sampling error. We refer to the process for choosing the sample as the sampling method. The process of learning about a population from a sample is called inference. The key to inference is determining the likely size of the sampling error. Exercise 4.1 Define the following terms: population sample parameter statistic sampling method error inference Solution the collection of cases of interest a subset of the population for which we gather data the numerical summary of the population (e.g., average, percent) the numerical summary of the sample (e.g., average, percent) the process for choosing the sample the difference between the sample statistic and the population parameter the process of learning about a population from a sample 4.2 The Simple Random Sample The easiest way to make inferences about the population from a sample is to draw your sample from the population at random without replacement. We call this a simple random sample. With a simple random sample, we randomly select individuals from the population without replacement, so each individual in the population is equally-likely to be included in the sample. 4.2.1 The Accuracy of the Simple Random Sample The simple random sample is quite accurate (i.e., the error is small). We can use the numbered-ticket model to establish the following result, which I discuss more thoroughly next week: The sample proportion will be the population proportion give or take [the SE] or so. Recall that the \\(\\text{SE} = \\dfrac{\\text{SD of tickets in the box}}{\\sqrt{\\text{number of draws}}}\\). Because the statement above is about a proportion, we should use a 0-1 box. However, the number of 0s and 1s that should be in the box isn’t clear because I haven’t described the population. It turns out that a box with a single 0 and a single 1 is the worst case, so let’s use that. A little experimentation makes it clear that 50% 0s and 50% makes the SD (and therefore the SE) as large as possible. Therefore, we can think of a box with a single 0 and a single 1 as the “worst case.” The SD of the tickets in this box is 0.5. The figure below uses this worst-case numbered-ticket model to compute the SE for the sample proportion as the sample size varies. It’s clear that the SE decreases as the sample size increases, but notice that it gets small quickly–you don’t need an enormous sample size to have a precise result. Notice that for a sample size of 625, the SE is 0.02 (or two percentage points). For a sample size of 2,500, the SE is 0.01 (or one percentage point). That is, the sample percentage will usually (i.e., 68% of the time) fall within two percentage points of the population percentage for a modest sample size of 625 people. For a sample size of 2,500, this shrinks to one percentage point. sample_size &lt;- c(100, 144, 196, 256, 324, 400, 1000, 1600, 2500) worst_case_sd &lt;- 0.5 worse_case_se &lt;- 0.5/sqrt(sample_size) data &lt;- data.frame(sample_size, worse_case_se) ggplot(data, aes(x = sample_size, y = worse_case_se)) + geom_line() 4.2.2 The Importance of the Simple Random Sample As I discuss below, the simple random sample is difficult to implement in practice. Why, then, do I emphasize it so heavily? First, it’s easy and powerful, so when it’s possible to implement, the simple random sample is a great choice. Second, because it’s simple, the simple random sample is easy to understand. As the sampling method becomes more complicated, the analysis becomes more technical and less intuitive–it’s no longer clear what it all means. However, the intuitions that we develop from a good understanding of the simple random sample generalize to other methods, even as the details change. 4.3 The Three Hurdles While researchers can easily predict the error for simple random samples in theory, three hurdles arise in practice. Selection bias occurs when the researcher cannot enumerate the population or develop a method to sample randomly from the population. Non-response bias occurs when the researcher cannot get everyone in the sample to respond to the survey. Other biases occur when a researcher cannot get respondents to answer the questions honestly, potentially because of a poorly designed questionaire. For example, if you wanted to randomly select 400 adults living in the US, how would you do it? The procedure isn’t immediately clear. And how would you convince those 400 folks to take the time to answer your questions? And how could you get them to honestly report their income and whether they voted? All of these difficulties increase the error, but none are reflected in the SE above. It’s possible to assess the likely sizes of these errors, but difficult. Because of these other important sources of error, we should distinguish between two sources of error. sampling error: the error due to sampling at random. If we repeated the same study again-and-again using a random sample, we’d get a slightly different result each time. This is due to sampling error. non-sampling error: errors not due to sampling at random, such as selection bias, non-response bias, and poor questionnaire design. Usually, the SE reflects only sampling error and not other (sometimes larger) sources of error. You can think of the SE as the lower-bound on the likely error. 4.4 Good Alternatives to the Simple Random Sample In a simple random sample, every member of the population has an equal chance of being selected into the sample. Equal chance of selection simplifies the analysis greatly, but it’s not critical that everyone has an equal chance of selection. It is crucial, though, each person’s chance of selection is known. We refer to sampling methods with known chances of selection as probability samples. We can develop a valid procedure to draw inferences about the population from probability samples. As long the chance of selection is known, then we can compute the sampling error. The analysis might be more complex, but it’s equally compelling. In some cases, these alternatives might help overcome selection bias or non-response bias, but in many cases, they do not. The hurdles for the simple random sample are the same for the other probability samples. Here are four examples of probability samples: simple random sample: The simple random sample has known probabilities of selection. It’s just 1/(population size) for every member of the population. systematic sample: List the individuals in the population (e.g., phone book student directly) and select every kth item on the list. Choose k to obtain the desired sample size. If you select the first item at random, then every individual has a known and equal chance of selection. stratified sampling: Break the population into several groups or “strata” and sample separately from each of these groups before combining them into a single collection. multistage cluster sampling: Conceptualize members of the population as being nested within hierarchies. Americans, for example, are nested within states, counties, and households. The researcher could select randomly select a state, then randomly select a county within that state, randomly select 10 households (addresses) in that county, and randomly select one person from each household. The researcher could repeat this 10 times to obtain a multistage cluster sample of 100 adults living in the US. This is useful when we cannot generate a list of adults living in the US. Using our knowledge of the number of people living in each county, the number of households in each county, and the number of people in the houses we select, we can compute the probability that each person is included in the sample. For all these probabilities samples, we can make valid, compelling inferences from the sample to the population. However, the analysis of the simple random sample is the simplest. It’s also conceptually similar to the other methods. 4.5 Poor Alternatives to the Simple Random Samples There are other common ways to collect a sample that do not use random selection with known selection probabilities. We refer to sampling methods that do not use random selection with known selection probabilities as nonprobability samples. Nonprobability samples can be helpful, but they do not allow us to easily predict the sampling error. This makes it hard to draw inferences about the larger population of interest. Here are a few examples of nonprobability samples: snowball sample: Might be used with hard to identify populations, such as drug users or sex workers. The researcher identifies an initial respondent to interview. The researcher asks the initial respondent and subsequent respondents to identify other potential respondents. convenience sample: Might be used in the initial phase of a research project for pre-testing or pilot studies. The researcher chooses this sample for convenience (usually cost). Rather than pay hundreds of dollars for a national sample, the researcher might use students at their university or MTurk workers, for example. quota sample: Breaks the population into several groups (e.g., women over 60, etc.) and samples a particular number or “quota” from each using a nonprobability method to obtain the desired sample size. While these samples are easy to generate in practice, the researcher cannot easily predict the error. 4.6 An Infamous Sample Survey Perhaps the most famous poll is the 1936 Literary Digest poll of the 1936 presidential election. In their poll of 10 million subscribers before the election, 1,293,669 (55%) preferred Landon and 972,897 (41%) preferred Roosevelt. On Election Day, Roosevelt got 61% and Landon got 37%. Exercise 4.2 Read the Literary Digest’s summary of their straw poll. Why do they have such confidence in their poll? Find several sentences where they boast about their extraordinary study. Using these sentences as evidence, what does the Digest think makes the poll accurate? Hint Is it the method for choosing the sample or the size of the sample? They never say “our sample is accurate because…,” but they strongly imply their (perhaps flawed) logic in several places. 4.7 Modern Polling Modern polls work to achieve accurate results through two methods: Design a study that mimics a simple random sample as closely as practical. Use tools to adjust the sample statistics after the data collection to more closely mimic the target population. Studies vary in their relative efforts. Some organizations spend hundreds or thousands of dollars per interview to closely mimic the simple random sample. Other organizations emphasize careful adjustments after collecting the data. Most studies use a mixture of the two. Exercise 4.3 YouGov is a reputable polling company that’s popular in political science. Read Nate Cohn’s description of YouGov’s approach here (or here as a backup). Summarize the difference between YouGov’s approach and the simple random sample. Why doesn’t YouGov just use a simple random sample? Exercise 4.4 The Annual Review of Political Science publishes invited articles in which established scholars discuss large bodies of research in political science. As such, the Annual Review of Political Science offers a wonderful resource when introducing yourself to research on a particular topic. Adam Berinsky recently wrote about public opinion polling. Read Berinsky (2017) and answer the following question based on the reading: Why is public opinion polling important in a democracy? Up to roughly 2000, public opinion polls relied on telephone surveys using random digit dialing and face-to-face interview using multistage cluster sampling. How has public opinion polling changed since? What problems remain? What is the “best” way to measure opinion? In particular, answer these two questions and explain. Should the researcher use one question or several questions? Should the researcher use a high, medium, or low level of abstraction? (The “best” approach might change depending on the context, but, for this exercise, take a position and defend it.) 4.8 Summary We study a small sample to learn about a large population. We refer to features of the population as parameters and features of the sample as statistics. We use sample statistics to learn about population parameters. In sample surveys, there’s one central question: how close are the sample statistic and the population parameter? Simple random samples work extremely well and are simple to analyze. Other probability samples can improve on simple random samples but are much more complex to analyze. Nonprobability methods do not allow the researcher to easily predict the sampling error. The method of generating the sample is more important than the size of the sample. In practice, it can be difficult to generate an appropriate sampling frame, obtain a high response rate, and ask good survey questions. In practice, modern survey methods are quite complicated. However, we can use the simple random sample as a model to understand and interpret results from much more complicated designs. "],["confidence-intervals.html", "Chapter 5 Confidence Intervals 5.1 Review 5.2 The Motivating Question 5.3 Reversing the Logic 5.4 Using the Logic 5.5 Confidence Intervals 5.6 Miscellaneous 5.7 Summary", " Chapter 5 Confidence Intervals 5.1 Review Remember, we can use the numbered-ticket model to understand simple random samples from a large population. From a couple of weeks ago, we learned the following: The numbered-ticket model says: Fill a box with \\(k\\) tickets numbered with \\(t_1\\), \\(t_2\\), \\(t_3\\), …, \\(t_k\\), draw \\(N\\) times with replacement from the box, and record the average-of-the-draws. The NTM is a particularly useful chance process to understand well, because we can choose the number of tickets, the numbers on those tickets, and the number of draws to make it “like” many other chance processes, especially sample surveys! The expected value is the long-run average of the average-of-the-draws. It’s equal to the average of the tickets in the box. The standard error (SE) is the long-run SD of the average-of-the-draws. It’s equal to \\((\\text{SD of the tickets in the box})/\\sqrt{({\\text{number of draws}})}\\). If you have a large number of draws, then the long-run histogram of the average-of-the-draws follows the normal curve. We convert to standard units or z-scores using the expected value in place of the average and the SE in place of the SD. We can use this long-run histogram to compute the chance that the average-of-the-draws will fall into particular ranges. When working with a 0-1 box (i.e., a box with only 0s and 1s), it’s helpful to remember two tricks: (1) the average of the tickets in the box equals the proportion of tickets that are 1s and (2) the SD of the tickets in the box equals \\(\\sqrt{(\\text{avg. of tickets in the box}) \\times (1 - \\text{avg. of the tickets in the box})}\\). Exercise 5.1 250 million adults are living in the US. 100 million “approve” of the job Donald Trump is doing as president, 140 million “disapprove,” and 10 million “aren’t sure.” Suppose I sample 400 of these adults and compute the proportion of people that approve. About 95% of the time, the proportion of the sample that approves will be between _________ and _________. (Round to three decimal places.) If you need a more thorough review of these ideas, see Chapter 3 and especially Exercise 3.21. Hint Choose an NTM that “is like” this chance process. Solution See Exercise 3.21. 5.2 The Motivating Question Motivating Question: I randomly sample 400 adults living in the US and 184 of those approve of the job Donald Trump is doing as president. Based on this simple random sample, what can I say about the entire population of adults living in the US? We’ve been using the numbered-ticket model to study questions much like this–finding the expected values, standard error, etc. Exercises 5.1 and 3.21 seem similar to this question. But the motivating question for this chapter is different in an important way: it’s backward. In Exercises 5.1 and 3.21, we reasoned forward from the population to the sample. Given the population, we used the numbered-ticket model to describe what the random samples would look like. In the motivating question above, I give you the sample and ask you what the population looks like. In short, we need to take the logic of Chapter 3 and reverse it. 5.3 Reversing the Logic We start with this statement that we developed in Chapter 3: The average-of-the-draws will be about the expected value, give or take an SE or so. We also have this variation: The average-of-the-draws will be within two SEs of the expected value about 95% of the time. We know these are true, so let’s modify them in ways that leave them true, but make them more applicable. 5.3.1 Replace NTM Terms with Sample Survey Terms First, we already know that, because of the indicator trick, the average-of-the-draws in the NTM corresponds to the sample proportion in sample surveys. The average-of-the-draws is just the proportion of 1s drawn, and it’s just like the proportion of approvers in the sample survey. Second, we already know that the expected value is the population proportion. Because we’re interested in a proportion, we use a 0-1 box and let the 1s represent the items that count (e.g., approvers). The expected value is simply the proportion of 1s in the box. Because we chose the proportion of 1s in the box to match the proportion of 1s in the population, the expected value is the proportion of 1s in the population. See Exercise 3.14 if you need a refresher. We can rewrite the two statements above replacing the phrase “average-of-the-draws” with “sample proportion” and the phrase “expected value” with “population proportion” The sample proportion will be about the population proportion, give or take an SE or so. The sample proportion will be within two SEs of the population proportion about 95% of the time. 5.3.2 The Symmetry of Distance Imagine you are playing golf and figure out that you are 100 yards from the hole. How far is the hole from you? 100 yards. It works the same for the sample proportion and population proportion. If the sample proportion is, say, 1.5 SEs, away from the population proportion, then the population proportion is 1.5 SEs away from the sample proportion. This is because distance is symmetric. If the sample proportion is “close” to the population proportion, then the population proportion cannot be “far” from the sample proportion. They are both “close” to each other or not. Indeed, they must be the same distance apart. Since both statements we’re interested in address the distance between the sample proportion and the population proportion, we can just switch their positions. The population proportion will be about the sample proportion, give or take an SE or so. The population proportion will be within two SEs of the sample proportion about 95% of the time. 5.3.3 Tense Lastly, we need to address the tense of the statement. When we started, we wanted to predict a future sample (or samples) based on a population that exists in the present. Now, we want to learn about a population that exists in the present based on a sample that we already have. So let’s switch from the future tense “will be” to the present tense. “is.” The population proportion is about the sample proportion, give or take an SE or so. The population proportion is within two SEs of the sample proportion about 95% of the time. Exercise 5.2 Start with the statement: “The average-of-the-draws will be within two SEs of the expected value about 95% of the time.” Get to the statement: “The population proportion is within two SEs of the sample proportion about 95% of the time.” Explain each of the following steps using your own language: Go from the language of the numbered-ticket model (e.g., “average-of-the-draws”, “expected value”) to the language of sample surveys (e.g., “sample” and “population”). Use the symmetry of distance. Change the tense. 5.4 Using the Logic Now we have two powerful statements that allow us to reason “backward” (actually the only direction we ever really cared about) from the sample to the population. The population proportion is about the sample proportion, give or take an SE or so. The population proportion is within two SEs of the sample proportion about 95% of the time. We already know that the population proportion will be “close” to the sample proportion. But how close? These two statements allow us to say precisely–we just need to compute the SE… 5.4.1 A Big Problem Remember that when we represent the sample survey as an NTM, then \\(\\text{SE} = \\dfrac{\\text{SD of tickets in the box}}{\\sqrt{\\text{number of draws}}}\\). Remembering that we’re dealing with a 0-1 box (because this is a proportion), then \\(\\text{SE} = \\dfrac{\\sqrt{\\text{avg. of tickets in the box} \\times (1 - \\text{avg. of tickets in the box})}}{\\sqrt{\\text{number of draws}}}\\). Now let’s convert the language from that of an NTM to that of a sample survey. First, the average of the tickets in the box is simply the population proportion (that’s how we chose the number of 0s and 1s for the box). Second, we might call the “number of draws” the “sample size.” Remember that the sample survey and the NTM are the same chance process (if we chose the correct NTM), so we’re not changing anything—these different words express the same ideas. \\(\\text{SE} = \\dfrac{\\sqrt{\\text{population proportion} \\times (1 - \\text{population proportion})}}{\\sqrt{\\text{sample size}}}\\). All we have to do is calculate this. Do you see the problem? There’s a big problem. We don’t know the population proportion. And our ignorance is fundamental—it’s the whole reason we’re doing the sample survey. To use the two statements, we need to know the SE. To know the SE, we need to know the population proportion. We’re stuck. 5.4.2 A Solution We don’t know the population proportion—I readily and fully admit that. But we’re backed into a corner. If I forced you to guess, what would you choose? Using the best information you have, what would you guess the population proportion to be? Well, our statement tells us that the “population proportion is about the sample proportion.” Can we just “plug-in” the sample proportion for the population proportion? Would that work in a pinch? It turns out that using the sample proportion instead of the population proportion works quite well. Methodologists sometimes refer to this approach as the “plug-in principle.” It turns out that this estimated SE is approximately the same as the actual SE. Notice now that we have two different SEs: the actual (that we can only know if we know the population proportion) and the estimated (that we use as a best-guess of the actual). We can indicate the estimated SE with a hat or refer to it as the “estimated SE.” (In practice, authors don’t do a good job of distinguishing between the actual and estimated SE. Usually, they are referring to the estimate, but you need to rely on context if the difference is important.) \\(\\text{estimated SE} = \\hat{\\text{SE}} = \\dfrac{\\sqrt{\\text{sample proportion} \\times (1 - \\text{sample proportion})}}{\\sqrt{\\text{sample size}}}\\). It’s not immediately clear, though, how well the estimated SE will work. Is it a crude approximation? Or is it almost exact? Without going into the details, I’ll just state the important conclusion: except for extremely small sample sizes or extremely large or small proportions, it works very well. If your sample has four people, it won’t work well. If you’re estimating the proportion of adults that make more than $1 million per year with a sample of 1,000 people, it won’t work well. But for most sample surveys, the approximation works well. Exercise 5.3 Suppose I sampled 64 respondents and obtained a sample proportion of 0.297. Estimate the SE for the sample proportion. Round to three decimal places. Solution estimated SE = sqrt(sample proportion x (1 - sample proportion))/sqrt(sample size) estimated SE = sqrt(0.297 x 0.703)/sqrt(64) estimated SE = sqrt(0.209)/8 estimated SE = 0.457/8 estimated SE = 0.057 Exercise 5.4 Suppose I sampled N respondents and obtained a sample proportion of p. Estimate the SE for the following values of N and p. N p Estimated SE 100 0.5 25 0.3 1000 0.2 387 0.345 Solution See Exercise 5.3 for the process. N p Estimated SE 100 0.5 0.05 25 0.3 0.092 1000 0.2 0.013 387 0.345 0.024 Exercise 5.5 Suppose I sampled 64 respondents and 19 respondents had a quality of interest. Estimate the SE for the proportion of the respondents with the quality of interest. Round to three decimal places. Solution Begin by finding the sample proportion, which is 19/64 = 0.297. After this, the Exercise is just like Exercise 5.3. Exercise 5.6 Suppose I sampled N respondents and k respondents had a quality of interest. Estimate the SE for the proportion of the respondents with the quality of interest for the following values of N and k. N k Estimated SE 100 50 25 16 1000 750 387 203 Solution Begin by finding the sample proportion. After this, this Exercise is just like Exercise 5.3. N k p Estimated SE 100 50 0.5 0.05 25 16 0.64 0.096 1000 750 0.75 0.014 387 203 0.525 0.025 Exercise 5.7 Imagine tossing a coin 25 times and computing the proportion of heads. What’s the expected value? What’s the (actual) SE? Now actually toss a coin 25 times and use the proportion of heads that you obtain to estimate the SE. Is the estimated SE close to the actual SE? Solution Tossing a coin 25 times and computing the proportion of heads is like… Drawing 25 times with replacement from a box with a single ticket numbered with 1 and a single ticket numbered with 0 and averaging the draws. expected value = 0.5 (i.e., the proportion of 1s in the box) SE = (SD of tickets in the box)/sqrt(number of draws) = sqrt(0.5 x 0.5)/sqrt(25) = 0.5/5 = 0.1 The actual SE is 0.5. I tossed a coin 25 times and I got 10 heads. That’s a sample proportion of 10/25 = 0.4. The estimated SE is sqrt(0.4 x (1 - 0.4))/sqrt(25) = 0.4898979/5 = 0.09797958 (or 0.10 after rounding). The estimated and actual SEs are very close. 5.5 Confidence Intervals Consider the second statement of interest: The population proportion is within two SEs of the sample proportion about 95% of the time. That gives us an interval: \\(\\text{95% confidence interval} = \\text{sample proportion} \\pm 2 \\times \\hat{\\text{SE}}\\). That is, we start at the sample proportion and hop 2 SE-hats to the left and 2 SE-hats to the right. This interval will “cover” or “capture” the population proportion 95% of the time. We can write a confidence interval as “[0.4, 0.6]” or “0.5 +/- 0.1” (read as “0.5 plus-or-minus 0.1”). Exercise 5.8 This continues Exercise 5.4.Suppose I sampled N respondents and obtained a sample proportion of p. Compute the 95% confidence interval for the following values of N and p. Round to three decimal places. Write the confidence interval in both formats. N (sample size) p (sample proportion) Estimated SE 95% Confidence Interval 100 0.5 __________ or 0.5 +/- ________ 25 0.3 __________ or 0.3 +/- ________ 1000 0.2 __________ or 0.2 +/- ________ 387 0.345 __________ or 0.345 +/- ________ Solution See Exercise 5.3 for the process of estimating the SE. You compute the estimated SE for each combination in Exercises 5.4. Simply add and subtract two estimated SEs to obtain the confidence interval. For the first combination, we have an estimated SE of 0.05, so the 95% confidence interval is [0.5 - 2 x 0.05, 0.5 + 2 x 0.05] = [0.5 - 0.1, 0.5 + 0.1] = [0.4, 0.6] or 0.5 +/- 0.1. N p Estimated SE 95% Confidence Interval 100 0.5 0.05 [0.4, 0.6] or 0.5 +/- 0.1 25 0.3 0.092 [0.117, 0.483] or 0.3 +/- 0.183 1000 0.2 0.013 [0.175, 0.225] or 0.2 +/- 0.025 387 0.345 0.024 [0.297, 0.393] or 0.345 +/- 0.048 5.5.1 Understanding a Confidence Interval To understand a confidence interval, you have to imagine repeating the study again-and-again a large number of times. Each study will give you a difference sample proportion, SE-hat, and confidence interval. Many of these confidence intervals will contain the population proportion, but some will not. In the long run, our method will capture the population proportion 95% of the time. To illustrate, I had my computer repeat a “study” 10,000 times. I repeatedly conduct a simple random sample on a population with 250 million people and 100 million approvers. For each “study,” I randomly sampled 400 “respondents” and computed the 95% confidence interval. The table below shows the results for the first 10 “studies.” In Study #6, 182 of the 400 respondents reported that they “approve.” This gives a sample proportion of 0.455. We use this 0.455 to estimate an SE of 0.025, which gives us a 95% confidence interval of [0.405, 0.505]. This confidence interval does not capture the population proportion, which is 0.40. But it’s the only one of the ten studies that does not. Study Population Proportion Number of Approvers in Sample of 400 Sample Proportion Estimated SE 95% Confidence Interval Capture? Study #1 0.4 170 0.425 0.025 [0.376, 0.474] Yes Study #2 0.4 150 0.375 0.024 [0.327, 0.423] Yes Study #3 0.4 148 0.370 0.024 [0.322, 0.418] Yes Study #4 0.4 161 0.402 0.025 [0.353, 0.452] Yes Study #5 0.4 146 0.365 0.024 [0.317, 0.413] Yes Study #6 0.4 182 0.455 0.025 [0.405, 0.505] No Study #7 0.4 148 0.370 0.024 [0.322, 0.418] Yes Study #8 0.4 163 0.408 0.025 [0.358, 0.457] Yes Study #9 0.4 179 0.448 0.025 [0.398, 0.497] Yes Study #10 0.4 165 0.412 0.025 [0.363, 0.462] Yes There isn’t room to show more than 10 studies in the table, but we can use a graph to show a lot more. The graph below shows the confidence intervals for the first 200 studies. The confidences intervals in red capture the population proportion, the confidence intervals in green miss high, and the confidence intervals in blue miss low. Three studies miss high, and four studies miss low. Thus, 193 of the 200 (96.5%) studies shown produce confidence intervals that capture the true value. In the long run, we expect about 95% of the studies to produce confidence intervals that capture the true value. Although I don’t show all 10,000 sample surveys that I conducted on my computer, I can tell you that 9,538 of those 10,000 studies produces confidence intervals that captured the true value. That’s 95.4%. Our method works very well. Exercise 5.9 Suppose you and I did identical surveys measuring the proportion of adults living in the US that approve of the job Donald Trump is doing as president. Suppose we both collect our own samples using the simple random sampling method. Which of the following will be different, and which will be the same? number of adults living in the US that approve proportion of adults that approve sample size number of approvers in the sample proportion of approvers in the sample actual SE estimated SE 95% confidence interval Solution We’re studying the same population, so all population parameters will be the same, including the number of approvers. We’re studying the same population, so all population parameters will be the same, including the proportion of approvers. We’re using identical methods, so our sample size will be the same. Because we’re using random samples, you and I will (almost certainly) have a different number of approvers in our samples. Because we’re using random samples, you and I will (almost certainly) have a different proportion of approvers in our samples. The actual SE only depends on the population proportion, which doesn’t change, so the actual SE is the same. Because the estimated SE uses the sample proportion, which will (almost certainly) be different, we will (almost certainly) have different estimated SEs. However, these will be quite close. Because we have different (but close) estimated SEs and different sample proportions, we will have different confidence intervals. 5.5.2 Interpreting a Confidence Interval We’ve just seen that a confidence interval doesn’t always contain the population proportion. It only captures the population proportion about 95% of the time. Because it sometimes misses, it takes courage to interpret a confidence interval. You interpret a confidence interval by making a bold claim: “The population proportion is in the confidence interval.” Specifically, you might say: “The proportion of adults living in the US that approve of the job Donald Trump is doing as president is between 0.41 and 0.51.” In the long-run, you’ll be wrong about 5% of the time. Those are errors—you’ve made a claim that isn’t true. Confidence intervals require courage because you’ve made yourself vulnerable to being wrong. You are not allowed to hedge: you cannot say it’s “probably” in there. You must say it “is” in there. Why? Why must you be so bold? The nuances and subtleties are a little beyond this class, but our approach is sometimes called “error statistics.” In error statistic, one develops a method (e.g., a confidence interval) that guarantees an acceptable error rate (e.g., 5% of the time, the confidence interval misses). If you hedge, then you’re not making an error. You need to interpret a confidence interval so that if that confidence interval misses, you were wrong. If you say that the confidence interval “probably” contains the population proportion, and it turns out to not, then you’re not really wrong. You haven’t really made an “error,” because you hedged. So here’s how you interpret a confidence interval. Be bold. Say that the confidence interval contains the population parameter. Realize that you will be wrong 5% of the time in the long run. That’s an acceptable error rate, though. Note: Many researchers still like to hedge—it’s just human nature—so they write something like: “I’m 95% confidence that the proportion of adults living in the US that approve of the job Donald Trump is doing as president is between 0.41 and 0.51.” I shrug a bit at this, but these researchers say that “95% confidence” has a technical meaning. It supposedly means that “I’m saying the parameter is definitely in there, but recognizing that I’m wrong about 95% of the time.” It doesn’t do anyone any good when the common understanding of 95% confidence differs from the technical meaning, so let’s be bold. Say what you mean: “It’s in there.” Accept that you’ll be wrong about 5% of the time in the long run. Exercise 5.10 You conduct a simple random sample to estimate the proportion of adults living in the US that identify as a Republican. You get a 95% confidence interval of [0.25, 0.35]. Which of the following is an acceptable way to interpret this interval? The proportion of adults living in the US that identify as a Republican is between 0.25 and 0.35. There’s a 95% chance that the proportion of adults living in the US that identify as a Republican is between 0.25 and 0.35. Solution Good. You’ll be wrong about 95% of the time in the long-run, but we’ve designed the method around this acceptable error rate. This is not how you interpret a confidence interval. This interpretation is appealing, but it’s wrong for technical reasons that I’m skimming over. 5.6 Miscellaneous There are confidence intervals besides a 95% confidence interval. You can also compute a 99% or a 90% confidence interval, for example. As a discipline, though, political science has decided that 5% is an acceptable error rate, so we use 95% confidence intervals. If you want a different interval, then find the z associated with the interval you want (2 for a 95% interval, 1.64 for a 90% interval, 2.5 for a 99% interval, etc.) and hop that many SEs left and right of the sample proportion. The method that we discussed works only for a simple random sample. Fortunately, the intuition generalizes to other scenarios. Unfortunately, the math does not. However, it works decently for most probability samples without any change, so it’s a good first approximation. The confidence interval only accounts for sampling error. Other sources of error, such as selection bias, response bias, and poorly worded survey questions are ignored. These other forms of error are perhaps more important, and the 95% confidence interval does not account for them. 5.7 Summary In previous chapters, we learned how to use the numbered-ticket model to understand unfamiliar chance processes, including the sample proportion from sample surveys. In this chapter, we reverse the logic to understand how close our sample proportion is to the population proportion. The primary method of inference is a 95% confidence interval, which (by design) captures the population parameter 95% of the time in the long run. To make sense of this, you need to imagine conducting your study repeatedly. The 95% confidence interval that we learned only works for the simple random sample and does not account for selection bias, response bias, and poorly worded survey questions. "],["hypothesis-testing.html", "Chapter 6 Hypothesis Testing 6.1 The Null Hypothesis 6.2 p-Values 6.3 A Summary of the Hypothesis Testing Framework", " Chapter 6 Hypothesis Testing As an alternative (or supplement) to confidence intervals, political scientists sometimes use p-values to test specific hypotheses. 6.1 The Null Hypothesis In the hypothesis testing framework, the researcher establishes two hypotheses: A null hypothesis describes the scenarios that the researcher hopes to reject. If the researcher wants to argue in favor of a particular model or theory, then the null hypothesis describes scenarios inconsistent with the theory. An alternative hypothesis describes the scenarios that the researcher hopes to accept. If the researcher wants to argue in favor of a particular model or theory, then the alternative hypothesis describes scenarios consistent with the theory. I like to call the alternative hypothesis the “research hypothesis.” The researcher then examines the data and determines whether the data are consistent with the null hypothesis. If the data are inconsistent with the null hypothesis, then the researcher rejects the null hypothesis and concludes that the research hypothesis must be correct. Here’s an example. A pollster working for the Trump campaign reports a new poll to the president in which 520 of 1000 respondents reported that they approve of the job Donald Trump was doing as president. He exclaims: “Wonderful! More than half of Americans approve.” The pollster cautions: “No—more than half of the sample approves! That might be due to chance. The actual number might be 50% or less.” So then the president wants to test (and hopefully reject) the null hypothesis that the proportion of Americans that approve is less than or equal to 0.5. 6.2 p-Values To test a hypothesis, we compute a p-value. A p-value is the probability that we would observe data at least as extreme as the observed data if the null hypothesis is true. By “as extreme,” we mean data that are less consistent with the null hypothesis. Here’s what we have so far: Null Hypothesis: population proportion is less than or equal to 0.5. Research Hypothesis: population proportion is greater than 0.5. Observed Data: sample proportion is 0.52. To compute the p-value, we just need to compute the probability that we would get a sample of 0.52 or higher (i.e., less consistent with the null hypothesis), if the population proportion were 0.5 or lower. The “or lower” piece at the end is required because those values are part of the null hypothesis, but that makes the computation hard. To get around this, it helps to focus on one particular parameter. 6.2.1 Focusing the Null Hypothesis When the null hypothesis contains a range of possibilities (e.g., 0.5 or lower), we can simplify this by focusing on the most difficult value. In this case, the hardest to reject (the one most consistent with the sample proportion) is 0.5. This sounds tricky, but it’s not. Here are two guidelines for focusing the null hypothesis: If the sample statistic (e.g., sample proportion) equals a population parameter from the null hypothesis, then the p-value equals one. No further computation is needed. Skip right to the end. If the sample statistic (e.g., sample proportion) does not equal any population parameter from the null hypothesis, then find the parameter from the null hypothesis that’s closest to the sample statistic. That’s the only value you need to worry about, so I call it the “focused null hypothesis.” Once we have that value, there’s more work to be done. In our case, the sample statistic falls outside the null hypothesis, so let’s choose the parameter from the null hypothesis that’s closest to the sample proportion of 0.52. The closest parameter from the null hypothesis to 0.52 is 0.50. That’s the “focused null hypothesis.” 6.2.2 Computing the p-value for the Focused Null Hypothesis To compute the p-value now, we just need to compute the probability that we would get a sample of 0.52 or higher (i.e., less consistent with the null hypothesis) if the population proportion were 0.5. (By focusing the null hypothesis, we’ve eliminated the “or lower” piece.) We know from the numbered-ticket model that if the population proportion were 0.5 (i.e., the focused null hypothesis were true), then… the expected value of sample proportion would be 0.5 and the SE of the sample proportion would be \\(\\frac{\\sqrt{0.5 \\times 0.5}}{\\sqrt{1000}} = \\frac{0.5}{31.623} = 0.016\\). I don’t use the sample proportion 0.52 in the formula for the SE as I did with confidence intervals. Instead, I use the population proportion of 0.50, which I can do because I am assuming that the (focused) null hypothesis is true. (Notice the “if the population proportion were 0.5” part of the statement above.) It doesn’t matter much, but if I’m assuming that the population proportion is 0.5 to compute the p-value, I can improve the accuracy slightly by assuming it when computing the SE as well. Our observed sample proportion falls \\(\\frac{0.520 - 0.500}{0.016} = 1.250\\) SEs above the expected value. We know that the sample proportion follows the normal curve. Using the rules of the normal curve, we can figure out the probability that we will get a sample proportion greater than or equal to 0.52. It’s 0.105. That’s the p-value. But what do we do with this p-value? If the p-value is small enough, then we reject the null hypothesis in favor of the alternative. If the p-value is not small enough, then we cannot reject or accept either hypothesis. In the latter scenario, the data are consistent with both hypotheses, we cannot be confident that either is correct. By convention, political scientists have settled on 0.05 as the critical value. If the p-value is less than or equal to 0.05, then reject the null hypothesis in favor of the alternative. If the p-value is greater than 0.05, then acknowledge that either hypothesis might be correct. Much like a 95% confidence will fail to capture the population parameter 5% of the time, this test will incorrectly reject 5% of the time if the null hypothesis is true. In the long-run, you’ll incorrectly reject the null hypothesis at most 5% of the time if you use this method. 6.3 A Summary of the Hypothesis Testing Framework Write down a null hypothesis. This should be the hypothesis you want to reject. If the null hypothesis includes a range of population parameters, then you need to focus on one particular value using the rules discussed above. Compute the p-value, which is the probability that we observe data at least as extreme as the observed data if the null hypothesis is true. By “extreme,” I mean data that are less consistent with the null hypothesis. If the p-value is less than or equal to 0.05, then reject the null hypothesis in favor of the alternative. If the p-value is greater than 0.05, then acknowledge that either hypothesis might be correct. Exercise 6.1 A pollster working for the Trump campaign reports a new poll to the president in which 510 of 1,000 respondents reported that they approve of the job Donald Trump was doing as president. He exclaims: “Wonderful! More than half of Americans approve.” The pollster cautions: “No—more than half of the sample approves! That might be due to chance. The actual number might be 50% or less.” Compute the p-value for the null hypothesis that the proportion of Americans that approve is 0.5 or lower. Do you reject the null hypothesis? Solution First, identify the null hypothesis. The population proportion is 0.5 or lower. Second, focus the null hypothesis. Because the sample proportion 0.51 is not part of the null hypothesis (from 0.00 to 0.50), we just choose the parameter from the null hypothesis that’s closest to 0.51. Our focused null hypothesis is 0.5. Third, compute the p-value, which is the probability that we observe data at least as extreme as the observed data if the null hypothesis is true. For this problem, we consider proportions above 0.51 as “more extreme” than 0.51, since those are more different from 0.50 than 0.51. If the (focused) null hypothesis is true, then… the expected value of sample proportion would be 0.5 and the SE of the sample proportion would be \\(\\frac{\\sqrt{0.5 \\times 0.5}}{\\sqrt{1000}} = \\frac{0.5}{31.623} = 0.016\\). The observed sample proportion falls \\(\\frac{0.510 - 0.500}{0.016} = 0.625\\) SEs above the expected value. We know that the sample proportion follows the normal curve. Using the rules of the normal curve (and letting z = 0.65), we can figure out the probability that we will get a sample proportion greater than or equal to 0.51. It’s 0.26. That’s the p-value. Because the p-value is greater than 0.05, you cannot reject the null. You conclude that the data are consistent with both the null hypothesis and the alternative hypothesis. Exercise 6.2 Continuing 6.1. What if the poll was 530 of 1,000? Solution First, identify the null hypothesis. The population proportion is 0.5 or lower. Second, focus the null hypothesis. Because the sample proportion 0.53 is not part of the null hypothesis (from 0.00 to 0.50), we just choose the parameter from the null hypothesis that’s closest to 0.53. Our focused null hypothesis is 0.5. Third, compute the p-value, which is the probability that we observe data at least as extreme as the observed data if the null hypothesis is true. For this problem, we consider proportions above 0.53 as “more extreme” than 0.53, since those are more different from 0.50 than 0.53. If the (focused) null hypothesis is true, then… the expected value of sample proportion would be 0.5 and the SE of the sample proportion would be \\(\\frac{\\sqrt{0.5 \\times 0.5}}{\\sqrt{1000}} = \\frac{0.5}{31.623} = 0.016\\). The observed sample proportion falls \\(\\frac{0.530 - 0.500}{0.016} = 1.875\\) SEs above the expected value. We know that the sample proportion follows the normal curve. Using the rules of the normal curve (and letting z = 1.90), we can figure out the probability that we will get a sample proportion greater than or equal to 0.53. It’s 0.03. That’s the p-value. Because the p-value is less than 0.05, you reject the null. You conclude that the data are not consistent with the null hypothesis and therefore conclude that the alternative hypothesis is correct. Exercise 6.3 Continuing 6.1. What if the poll was 5,100 of 10,000? Solution First, identify the null hypothesis. The population proportion is 0.5 or lower. Second, focus the null hypothesis. Because the sample proportion 0.51 is not part of the null hypothesis (from 0.00 to 0.50), we just choose the parameter from the null hypothesis that’s closest to 0.51. Our focused null hypothesis is 0.5. Third, compute the p-value, which is the probability that we observe data at least as extreme as the observed data if the null hypothesis is true. For this problem, we consider proportions above 0.51 as “more extreme” than 0.51, since those are more different from 0.50 than 0.51. If the (focused) null hypothesis is true, then… the expected value of sample proportion would be 0.5 and the SE of the sample proportion would be \\(\\frac{\\sqrt{0.5 \\times 0.5}}{\\sqrt{10000}} = \\frac{0.5}{100} = 0.005\\). The observed sample proportion falls \\(\\frac{0.510 - 0.500}{0.005} = 2\\) SEs above the expected value. We know that the sample proportion follows the normal curve. Using the rules of the normal curve, we can figure out the probability that we will get a sample proportion greater than or equal to 0.51 if the null hypothesis is true. It’s 0.025. That’s the p-value. Because the p-value is less than 0.05, you reject the null. You conclude that the data are not consistent with the null hypothesis and therefore conclude that the alternative hypothesis is correct. Exercise 6.4 A pollster working for the Biden campaign reports a new poll to the candidate in which 490 of 1,000 respondents reported that they approve of the job Donald Trump was doing as president. He exclaims: “Wonderful! Less than half of Americans approve.” The pollster cautions: “No—less than half of the sample approves! That might be due to chance. The actual number might be 50% or more.” Compute the p-value for the null hypothesis that the proportion of Americans that approve is 0.5 or higher. Do you reject the null? Solution First, identify the null hypothesis. The population proportion is 0.5 or higher. Second, focus the null hypothesis. Because the sample proportion 0.49 is not part of the null hypothesis (from 0.50 to 1.00), we just choose the parameter from the null hypothesis that’s closest to 0.49. Our focused null hypothesis is 0.5. Third, compute the p-value, which is the probability that we observe data at least as extreme as the observed data if the null hypothesis is true. For this problem, we consider proportions below 0.49 as “more extreme” than 0.49, since those are more different from 0.50 than 0.49. If the (focused) null hypothesis is true, then… the expected value of sample proportion would be 0.5 and the SE of the sample proportion would be \\(\\frac{\\sqrt{0.5 \\times 0.5}}{\\sqrt{1000}} = \\frac{0.5}{31.623} = 0.016\\). The observed sample proportion falls 0.625 SEs below the expected value, because \\(\\frac{0.049 - 0.500}{0.016} = -0.625\\). We know that the sample proportion follows the normal curve. Using the rules of the normal curve (and letting z = -0.65), we can figure out the probability that we will get a sample proportion less than or equal to 0.49 if the focused null hypothesis is true. It’s 0.26. That’s the p-value. Because the p-value is greater than 0.05, you cannot reject the null. You conclude that the data are consistent with both the null hypothesis and the alternative hypothesis. "],["extensions.html", "Chapter 7 Extensions 7.1 Review 7.2 Sources of Randomness 7.3 Confidence Intervals in General 7.4 Hypothesis Tests in General 7.5 Statistical Significance", " Chapter 7 Extensions 7.1 Review In this class, we’ve talked about how we can use concepts, models, measurements, and comparisons to carefully observe the political world. We spent a little time talking about concepts, models, and measurements. We spend most of the time talking about comparisons. We talked about percentages, proportions, averages, and standard deviations. We noted that we might compare these across groups. For example, we compared the average ideology score for Republicans across Congresses (they’ve been shifting to the right.) We compared the turnout rate among those who received the “self mailer” to the rate for those that received the “neighbors mailer” (threatening to expose non-voters to their neighbors causes them to vote). We talked about using correlation and regression to make comparisons directly. We examined Gamson’s Law (seat shares vary almost one-to-one with portfolio shares) and we examined the relationship between district magnitude and the effective number of parties (as magnitude goes up, so does the number of parties). We noted that there are four ways to get a relationship between two variables: Causation Spuriousness Reverse Causation Chance We talked about how we can use randomization (i.e., experiments) to eliminate the possibility that a correlation is due to spuriousness or reverse causation. I mentioned that we would deal with chance later in the semester. 7.2 Sources of Randomness At this point, we’ve used sample surveys to better understand how to deal with chance using confidence intervals and/or p-values. In our application, chance always entered the data through random sampling. In general, randomness enters our comparisons when we have any of the following: random sampling: the result differs from study to study because we get a different sample in each study. randomization: the result differs from study to study because we put different subjects in the treatment and control groups across the studies. imagining a stochastic world: if we rewound time and let the world play forward again, then idiosyncratic or “random” events would lead to different outcomes. The world we observe is just one of many possibilities. The observed world is like a random sample from the possible worlds. The last—a stochastic world—seems a bit weird. In almost all cases, political scientists assume that the data the real world gives us (e.g., the nominate data, the parties dataset) have a random component. They imagine, at least in practice, that we can think of some parts of the outcome as “systematic” or “explainable” and other parts as “idiosyncratic”, “random,” or “unexplainable.” For example, suppose that you get a flat tire on Election Day and can’t make it to the polling place. We might think of that as a random event (like a coin toss) that stopped you from voting. Thus, whether or not you voted is partly systematic (your education, political interest, etc.) and partly random (e.g., flat tire, medical emergency, etc.). Unlike random sampling or randomization, this source of randomness is totally imaginary. But if we imagine it, we can model it. 7.3 Confidence Intervals in General We focused our detailed applications narrowly on simple random samples and proportions. But instead of proportions, we could have focused on averages, SDs, correlations, or regression slopes. Instead of simple random samples, we could have focused on randomization or imagined randomness. The math would have been harder, but the logic is the same. Because of the randomness, the dataset you have helps you estimate the quantity you really care about, but with some error. We can use confidence intervals to determine how close the estimate is to the quantity we care about. Statisticians have developed many quantities to summarize a dataset and methods to estimate the SE for those quantities. Regardless of the quantity, the logic is always: 95% confidence interval = estimate +/- 2 SEs (There are other ways to create a confidence interval, but this approach is common.) The quantity we’re talking about might change from proportion to an average to a correlation to a slope to something you haven’t learned yet. The formula for the SE will change across these quantities. But the interpretation of the SE is always the same–the SE is the long run SD of the estimate if you repeat the study again-and-again. 7.4 Hypothesis Tests in General As with confidence intervals, our discussion of hypothesis tests focused narrowly on simple random samples and proportions. Also like confidence intervals, that logic generalizes. For a given null hypothesis about a particular quantity, statisticians have developed a method to compute the p-value. Regardless of the null hypothesis and regardless of the quantity of interest, the p-value is the probability that we would observe data at least as extreme as the observed data if the null hypothesis is true. If the p-value is less than or equal to 0.05, we reject the null hypothesis and accept the alternative. If the p-value is greater than 0.05, we acknowledge that the data are consistent with both the null and alternative hypotheses. 7.5 Statistical Significance When we’re comparing averages or examining a regression slope, it’s common for political scientists to use “no difference” or “no relationship” as the null hypothesis. It’s so common, in fact, that political scientists rarely explicitly state the null hypothesis. When it’s not stated, you can assume that the researcher is using the hypotheses below. When comparing groups with a proportion, percent, or average: null hypothesis: no difference alternative hypothesis: any difference (positive or negative) When comparing variables with a correlation or a variable: null hypothesis: no relationship alternative hypothesis: any relationship (positive or negative) When we reject the null hypothesis of no difference or relationship, we refer to the estimate as “statistically significant.” I don’t care for the term at all because “significant” seems to imply “important.” However, “statistically significant” is neither necessary nor sufficient for a statistical result to be important. Say that an estimate of a difference or a relationship is statistically significant if you can reject the null hypothesis of no difference or no relationship. In other words, when an estimate of a difference or relationship is statistically significant, you can confidently claim that the observed difference or relationship is not due to chance. It’s quite common to flag statistically significant estimates with stars. When you see an estimate flagged with a star, you know that it’s statistically significant. As an example, see Table 3 below from Gerber, Green, and Larimer (2008). The numbers are estimates of the treatment effect of each mailer (compared to the control group that received no mailer). Each estimate is statistically significant, which means we’re confident the observed differences are not due to chance. "],["appendix-a-normal-table.html", "A Appendix: A Normal Table", " A Appendix: A Normal Table z % between -z and z 0.00 0.00% 0.05 3.99% 0.10 7.97% 0.15 11.92% 0.20 15.85% 0.25 19.74% 0.30 23.58% 0.35 27.37% 0.40 31.08% 0.45 34.73% 0.50 38.29% 0.55 41.77% 0.60 45.15% 0.65 48.43% 0.70 51.61% 0.75 54.67% 0.80 57.63% 0.85 60.47% 0.90 63.19% 0.95 65.79% 1.00 68.27% 1.05 70.63% 1.10 72.87% 1.15 74.99% 1.20 76.99% 1.25 78.87% 1.30 80.64% 1.35 82.30% 1.40 83.85% 1.45 85.29% 1.50 86.64% 1.55 87.89% 1.60 89.04% 1.65 90.11% 1.70 91.09% 1.75 91.99% 1.80 92.81% 1.85 93.57% 1.90 94.26% 1.95 94.88% 2.00 95.45% 2.05 95.96% 2.10 96.43% 2.15 96.84% 2.20 97.22% 2.25 97.56% 2.30 97.86% 2.35 98.12% 2.40 98.36% 2.45 98.57% 2.50 98.76% 2.55 98.92% 2.60 99.07% 2.65 99.20% 2.70 99.31% 2.75 99.40% 2.80 99.49% 2.85 99.56% 2.90 99.63% 2.95 99.68% 3.00 99.73% 3.05 99.77% 3.10 99.81% 3.15 99.84% 3.20 99.86% 3.25 99.88% 3.30 99.90% 3.35 99.92% 3.40 99.93% 3.45 99.94% 3.50 99.95% 3.55 99.96% 3.60 99.97% 3.65 99.97% 3.70 99.98% 3.75 99.98% 3.80 99.99% 3.85 99.99% 3.90 99.99% 3.95 99.99% 4.00 99.99% 4.05 99.99% 4.10 100.00% 4.15 100.00% 4.20 100.00% 4.25 100.00% 4.30 100.00% 4.35 100.00% 4.40 100.00% 4.45 100.00% "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
